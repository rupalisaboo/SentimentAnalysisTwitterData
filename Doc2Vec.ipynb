{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from timeit import default_timer\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    print 'I\\'m here.' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    print 'I\\'m done.' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here.2015-12-09 12:47:26\n",
      "1.0\n",
      "I'm done.2015-12-09 12:47:31\n"
     ]
    }
   ],
   "source": [
    "with elapsed_timer() as elapsed:\n",
    "    time.sleep(1)\n",
    "    duration = '%.1f' % elapsed()\n",
    "    print duration\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3137 data/view1_clean\n",
      "    3137 data/view2_clean\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Split the all_views, get one file(sentiment, tweet) for each view\n",
    "#Clean each view\n",
    "#Run this for each view.\n",
    "norm() {\n",
    "    fn=$1\n",
    "    if [ ! -f \"$fn\" ]\n",
    "    then\n",
    "        echo \"File: $fn not found\"\n",
    "        exit\n",
    "    fi\n",
    "    #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
    "    function normalize_text {\n",
    "        awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
    "        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
    "        -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
    "    }\n",
    "    export LC_ALL=C\n",
    "    normalize_text \"$fn\"\n",
    "    wc -l $fn\n",
    "    mv \"$fn\" \"$fn-norm\"\n",
    "}\n",
    "norm \"data/view1_clean\" #file name is\n",
    "norm \"data/view2_clean\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "tw_view_1 = 'data/view1_clean-norm'\n",
    "tw_view_2 = 'data/view2_clean-norm'\n",
    "assert os.path.isfile(tw_view_1), tw_view_1 + \" unavailable\"\n",
    "assert os.path.isfile(tw_view_2), tw_view_2 + \" unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3137 docs: 2196 train-sentiment, 941 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple, defaultdict as dd\n",
    "\n",
    "#sentiment = {'positive':1, 'negative':-1} #, 'neutral':2}\n",
    "sentiment_dict = {'4':1, '0':-1} #- new data 0,4\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = dd(list)  # will hold all docs in original order - dictionary, keys = [v1, v2]\n",
    "v1 = 'view1'\n",
    "v2 = 'view2'\n",
    "#tw_sentiment_dict = {}\n",
    "#print total_num, train_test_shuffle\n",
    "all_v2_words = []\n",
    "with open(tw_view_2) as allview2:\n",
    "        all_v2_words = allview2.readlines()\n",
    "total_num = len(all_v2_words)\n",
    "#split train/test\n",
    "train_num = total_num *  7 / 10 # 70% train/test 1 - 10\n",
    "train_test_shuffle = np.arange(total_num)\n",
    "np.random.shuffle(train_test_shuffle)\n",
    "with open(tw_view_1) as allview1:\n",
    "    #for line_no, (v1, v2) in enumerate(zip(allview1, allview2)):\n",
    "    for line_no, line in enumerate(allview1):\n",
    "        tokens = gensim.utils.to_unicode(line).split('\\t')\n",
    "        if len(tokens) != 2:\n",
    "            print line\n",
    "            raise Exception()\n",
    "        sentiment = sentiment_dict[tokens[0]]\n",
    "        #if tw_id not in tw_sentiment_dict.keys():\n",
    "        #    continue\n",
    "        words = tokens[1]\n",
    "        split = 'train' if train_test_shuffle[line_no] <= train_num else 'test'\n",
    "        #sentiment = tw_sentiment_dict[tw_id]\n",
    "        v2_words = gensim.utils.to_unicode(all_v2_words[line_no]).split('\\t')[1]\n",
    "        \n",
    "        alldocs[v1].append(SentimentDocument(words, [line_no], split, sentiment))\n",
    "        alldocs[v2].append(SentimentDocument(v2_words, [line_no], split, sentiment))\n",
    "train_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'train'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'train']\n",
    "}\n",
    "test_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'test'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'test']\n",
    "}\n",
    "doc_list = { v1: alldocs[v1][:], v2: alldocs[v2][:] }  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list[v1]), len(train_docs[v1]), len(test_docs[v1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view1 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view1 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view1 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view2 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "model_size = 200\n",
    "simple_models , models_by_name = {}, {} \n",
    "for view in [v1, v2]:\n",
    "    simple_models[view] = [\n",
    "        # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "        Doc2Vec(dm=1, dm_concat=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "        # PV-DBOW \n",
    "        Doc2Vec(dm=0, size=model_size, negative=5, hs=0, min_count=5, workers=cores),\n",
    "        # PV-DM w/average\n",
    "        Doc2Vec(dm=1, dm_mean=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    ]\n",
    "\n",
    "    # speed setup by sharing results of 1st model's vocabulary scan\n",
    "    simple_models[view][0].build_vocab(alldocs[view])  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "    print view, simple_models[view][0]\n",
    "    for model in simple_models[view][1:]:\n",
    "        model.reset_from(simple_models[view][0])\n",
    "        print view, model\n",
    "\n",
    "    models_by_name[view] = OrderedDict((str(model), model) for model in simple_models[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "for view in [v1, v2]:\n",
    "    models_by_name[view]['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][2]])\n",
    "    models_by_name[view]['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][0]])\n",
    "#print models_by_name['dbow+dmm'], models_by_name['dbow+dmc'] \n",
    "#del models_by_name['dbow+dmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn import svm, metrics, neighbors\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "import ipdb\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    #ipdb.set_trace()\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def svm_predictor_from_data(train_targets, train_regressors):\n",
    "    svc = svm.SVC(kernel='rbf', degree=5, gamma=9e-1)\n",
    "    svc.fit(train_regressors, train_targets)\n",
    "    return svc\n",
    "\n",
    "    \"\"\"expected = svm_y_test\n",
    "    predicted = svc.predict(svm_x_test)\n",
    "\n",
    "    #print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "    #      % (svc, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    \"\"\"\n",
    "    \n",
    "def knn_predictor_from_data(train_targets, train_regressors):\n",
    "    k = 5\n",
    "    clf = neighbors.KNeighborsClassifier(k)\n",
    "    clf.fit(train_regressors, train_targets)\n",
    "    return clf\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    #train_regressors = sm.add_constant(train_regressors)\n",
    "    #predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "    predictor = svm_predictor_from_data(train_targets, train_regressors)\n",
    "    #predictor = knn_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_data]\n",
    "    #test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_data]\n",
    "    \"\"\"if not infer:\n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "              % (predictor, metrics.classification_report(expected, predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    corrects = sum(expected == predicted)\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "best_error = dd(lambda: dd(lambda :(1.0, 0.0)))  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started.\n",
      "======== view1 =========\n",
      "START 2015-12-10 12:48:01.724790\n",
      "*0.412327 : 1 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.9s\n",
      "*0.382979 : 1 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.5s\n",
      "*0.427205 : 1 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.1s\n",
      "*0.319149 : 1 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.9s\n",
      "*0.445271 : 1 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.0s\n",
      "*0.372340 : 1 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 1.7s\n",
      "*0.445271 : 1 passes : view1-dbow+dmm 0.0s 3.7s\n",
      "*0.468085 : 1 passes : view1-dbow+dmm_inferred 0.0s 2.8s\n",
      "*0.444208 : 1 passes : view1-dbow+dmc 0.0s 3.4s\n",
      "*0.446809 : 1 passes : view1-dbow+dmc_inferred 0.0s 2.6s\n",
      "completed pass 1 at alpha 0.025000\n",
      " 0.415515 : 2 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      " 0.433581 : 2 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.7s\n",
      "*0.445271 : 2 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 1.9s\n",
      "*0.445271 : 2 passes : view1-dbow+dmm 0.0s 3.6s\n",
      "*0.444208 : 2 passes : view1-dbow+dmc 0.0s 3.4s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.411265 : 3 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      " 0.430393 : 3 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      "*0.445271 : 3 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      "*0.445271 : 3 passes : view1-dbow+dmm 0.0s 3.6s\n",
      "*0.444208 : 3 passes : view1-dbow+dmc 0.0s 3.4s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.405951 : 4 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.8s\n",
      "*0.425080 : 4 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      "*0.445271 : 4 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      "*0.445271 : 4 passes : view1-dbow+dmm 0.0s 3.6s\n",
      "*0.444208 : 4 passes : view1-dbow+dmc 0.0s 3.4s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.403826 : 5 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      "*0.372340 : 5 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.4s\n",
      " 0.431456 : 5 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.1s 1.7s\n",
      " 0.372340 : 5 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.1s 1.4s\n",
      "*0.445271 : 5 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      " 0.372340 : 5 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.2s 1.5s\n",
      "*0.445271 : 5 passes : view1-dbow+dmm 0.0s 3.6s\n",
      "*0.457447 : 5 passes : view1-dbow+dmm_inferred 0.0s 2.8s\n",
      "*0.444208 : 5 passes : view1-dbow+dmc 0.0s 3.4s\n",
      "*0.404255 : 5 passes : view1-dbow+dmc_inferred 0.0s 2.6s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.402763 : 6 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.7s\n",
      " 0.426142 : 6 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.1s 1.7s\n",
      "*0.445271 : 6 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      "*0.445271 : 6 passes : view1-dbow+dmm 0.0s 3.5s\n",
      "*0.444208 : 6 passes : view1-dbow+dmc 0.0s 3.3s\n",
      "completed pass 6 at alpha 0.019000\n",
      "*0.401700 : 7 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.7s\n",
      " 0.435707 : 7 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.1s 1.7s\n",
      "*0.445271 : 7 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      "*0.445271 : 7 passes : view1-dbow+dmm 0.0s 3.5s\n",
      "*0.444208 : 7 passes : view1-dbow+dmc 0.0s 3.3s\n",
      "completed pass 7 at alpha 0.017800\n",
      " 0.415515 : 8 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.7s\n",
      "*0.425080 : 8 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.1s 1.7s\n",
      "*0.445271 : 8 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      "*0.445271 : 8 passes : view1-dbow+dmm 0.0s 3.6s\n",
      "*0.444208 : 8 passes : view1-dbow+dmc 0.0s 3.5s\n",
      "completed pass 8 at alpha 0.016600\n",
      " 0.409139 : 9 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      " 0.428268 : 9 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.1s 2.3s\n",
      "*0.445271 : 9 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.6s\n",
      "*0.445271 : 9 passes : view1-dbow+dmm 0.0s 5.1s\n",
      "*0.444208 : 9 passes : view1-dbow+dmc 0.0s 3.5s\n",
      "completed pass 9 at alpha 0.015400\n",
      " 0.404888 : 10 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      " 0.404255 : 10 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.4s 1.4s\n",
      " 0.428268 : 10 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      " 0.382979 : 10 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.4s\n",
      "*0.445271 : 10 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 2.0s\n",
      " 0.510638 : 10 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.2s 2.0s\n",
      "*0.445271 : 10 passes : view1-dbow+dmm 0.0s 6.4s\n",
      "*0.393617 : 10 passes : view1-dbow+dmm_inferred 0.0s 3.4s\n",
      "*0.444208 : 10 passes : view1-dbow+dmc 0.0s 4.3s\n",
      " 0.436170 : 10 passes : view1-dbow+dmc_inferred 0.0s 3.1s\n",
      "completed pass 10 at alpha 0.014200\n",
      " 0.402763 : 11 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.427205 : 11 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.4s\n",
      "*0.445271 : 11 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.9s\n",
      "*0.445271 : 11 passes : view1-dbow+dmm 0.0s 6.7s\n",
      "*0.444208 : 11 passes : view1-dbow+dmc 0.0s 4.4s\n",
      "completed pass 11 at alpha 0.013000\n",
      " 0.410202 : 12 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.0s\n",
      " 0.431456 : 12 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.4s\n",
      "*0.445271 : 12 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.7s\n",
      "*0.445271 : 12 passes : view1-dbow+dmm 0.0s 4.9s\n",
      "*0.444208 : 12 passes : view1-dbow+dmc 0.0s 4.2s\n",
      "completed pass 12 at alpha 0.011800\n",
      " 0.405951 : 13 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.5s\n",
      " 0.432519 : 13 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.4s\n",
      "*0.445271 : 13 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.6s\n",
      "*0.445271 : 13 passes : view1-dbow+dmm 0.0s 4.5s\n",
      "*0.444208 : 13 passes : view1-dbow+dmc 0.0s 4.4s\n",
      "completed pass 13 at alpha 0.010600\n",
      " 0.411265 : 14 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.6s 2.2s\n",
      " 0.432519 : 14 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.4s\n",
      "*0.445271 : 14 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.7s\n",
      "*0.445271 : 14 passes : view1-dbow+dmm 0.0s 4.7s\n",
      "*0.444208 : 14 passes : view1-dbow+dmc 0.0s 4.4s\n",
      "completed pass 14 at alpha 0.009400\n",
      " 0.408077 : 15 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.6s 2.1s\n",
      " 0.457447 : 15 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.6s 1.9s\n",
      " 0.431456 : 15 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      " 0.361702 : 15 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.6s\n",
      "*0.445271 : 15 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.457447 : 15 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 1.9s\n",
      "*0.445271 : 15 passes : view1-dbow+dmm 0.0s 4.9s\n",
      " 0.500000 : 15 passes : view1-dbow+dmm_inferred 0.0s 3.7s\n",
      "*0.444208 : 15 passes : view1-dbow+dmc 0.0s 4.4s\n",
      "*0.351064 : 15 passes : view1-dbow+dmc_inferred 0.0s 3.4s\n",
      "completed pass 15 at alpha 0.008200\n",
      " 0.403826 : 16 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.6s 2.2s\n",
      " 0.427205 : 16 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.445271 : 16 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      "*0.445271 : 16 passes : view1-dbow+dmm 0.0s 4.7s\n",
      "*0.444208 : 16 passes : view1-dbow+dmc 0.0s 4.4s\n",
      "completed pass 16 at alpha 0.007000\n",
      " 0.405951 : 17 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.426142 : 17 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.5s\n",
      "*0.445271 : 17 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.6s\n",
      "*0.445271 : 17 passes : view1-dbow+dmm 0.0s 4.7s\n",
      "*0.444208 : 17 passes : view1-dbow+dmc 0.0s 4.4s\n",
      "completed pass 17 at alpha 0.005800\n",
      " 0.404888 : 18 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.2s\n",
      " 0.428268 : 18 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.445271 : 18 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      "*0.445271 : 18 passes : view1-dbow+dmm 0.0s 4.7s\n",
      "*0.444208 : 18 passes : view1-dbow+dmc 0.0s 4.3s\n",
      "completed pass 18 at alpha 0.004600\n",
      " 0.404888 : 19 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.427205 : 19 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.445271 : 19 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.6s\n",
      "*0.445271 : 19 passes : view1-dbow+dmm 0.0s 4.7s\n",
      "*0.444208 : 19 passes : view1-dbow+dmc 0.0s 4.3s\n",
      "completed pass 19 at alpha 0.003400\n",
      " 0.404888 : 20 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.2s\n",
      " 0.436170 : 20 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.8s\n",
      " 0.428268 : 20 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      " 0.361702 : 20 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.7s\n",
      "*0.445271 : 20 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.478723 : 20 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 2.0s\n",
      "*0.445271 : 20 passes : view1-dbow+dmm 0.0s 4.6s\n",
      " 0.457447 : 20 passes : view1-dbow+dmm_inferred 0.0s 3.7s\n",
      "*0.444208 : 20 passes : view1-dbow+dmc 0.0s 4.4s\n",
      " 0.425532 : 20 passes : view1-dbow+dmc_inferred 0.0s 3.3s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2015-12-10 12:54:12.485516\n",
      "======== view2 =========\n",
      "START 2015-12-10 12:54:12.485578\n",
      "*0.450584 : 1 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      "*0.521277 : 1 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.8s\n",
      "*0.443146 : 1 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.478723 : 1 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.7s\n",
      "*0.450584 : 1 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.4s\n",
      "*0.468085 : 1 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 1.9s\n",
      "*0.449522 : 1 passes : view2-dbow+dmm 0.0s 4.6s\n",
      "*0.382979 : 1 passes : view2-dbow+dmm_inferred 0.0s 3.5s\n",
      "*0.450584 : 1 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "*0.457447 : 1 passes : view2-dbow+dmc_inferred 0.0s 3.5s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.450584 : 2 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.2s\n",
      "*0.438895 : 2 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 2 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      "*0.449522 : 2 passes : view2-dbow+dmm 0.0s 4.6s\n",
      " 0.451647 : 2 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 2 at alpha 0.023800\n",
      " 0.455898 : 3 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.2s\n",
      " 0.445271 : 3 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.2s\n",
      "*0.450584 : 3 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.4s\n",
      "*0.449522 : 3 passes : view2-dbow+dmm 0.0s 4.5s\n",
      " 0.451647 : 3 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.450584 : 4 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.449522 : 4 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 4 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.450584 : 4 passes : view2-dbow+dmm 0.0s 4.6s\n",
      " 0.451647 : 4 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 4 at alpha 0.021400\n",
      " 0.451647 : 5 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.2s\n",
      "*0.414894 : 5 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.8s\n",
      " 0.450584 : 5 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.2s\n",
      "*0.436170 : 5 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.8s\n",
      "*0.450584 : 5 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.468085 : 5 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 2.0s\n",
      " 0.450584 : 5 passes : view2-dbow+dmm 0.0s 4.9s\n",
      " 0.531915 : 5 passes : view2-dbow+dmm_inferred 0.0s 3.7s\n",
      " 0.451647 : 5 passes : view2-dbow+dmc 0.0s 4.4s\n",
      " 0.478723 : 5 passes : view2-dbow+dmc_inferred 0.0s 3.4s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.449522 : 6 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.448459 : 6 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 6 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.4s\n",
      " 0.450584 : 6 passes : view2-dbow+dmm 0.0s 4.6s\n",
      " 0.451647 : 6 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 6 at alpha 0.019000\n",
      " 0.452710 : 7 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.443146 : 7 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 7 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.450584 : 7 passes : view2-dbow+dmm 0.0s 4.7s\n",
      " 0.451647 : 7 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 7 at alpha 0.017800\n",
      "*0.449522 : 8 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.447396 : 8 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.2s\n",
      "*0.450584 : 8 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.450584 : 8 passes : view2-dbow+dmm 0.0s 3.8s\n",
      " 0.451647 : 8 passes : view2-dbow+dmc 0.0s 3.5s\n",
      "completed pass 8 at alpha 0.016600\n",
      "*0.448459 : 9 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      " 0.446334 : 9 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      "*0.450584 : 9 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      " 0.450584 : 9 passes : view2-dbow+dmm 0.0s 3.6s\n",
      " 0.451647 : 9 passes : view2-dbow+dmc 0.0s 3.4s\n",
      "completed pass 9 at alpha 0.015400\n",
      " 0.450584 : 10 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      "*0.382979 : 10 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.4s 1.4s\n",
      " 0.443146 : 10 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      "*0.425532 : 10 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.4s\n",
      "*0.450584 : 10 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 2.0s\n",
      " 0.478723 : 10 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.2s 1.6s\n",
      " 0.450584 : 10 passes : view2-dbow+dmm 0.0s 3.6s\n",
      " 0.446809 : 10 passes : view2-dbow+dmm_inferred 0.0s 2.9s\n",
      " 0.451647 : 10 passes : view2-dbow+dmc 0.0s 3.4s\n",
      "*0.414894 : 10 passes : view2-dbow+dmc_inferred 0.0s 2.7s\n",
      "completed pass 10 at alpha 0.014200\n",
      " 0.452710 : 11 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      " 0.441020 : 11 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      "*0.450584 : 11 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 2.0s\n",
      " 0.450584 : 11 passes : view2-dbow+dmm 0.0s 3.7s\n",
      " 0.451647 : 11 passes : view2-dbow+dmc 0.0s 3.4s\n",
      "completed pass 11 at alpha 0.013000\n",
      " 0.453773 : 12 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      " 0.442083 : 12 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      "*0.450584 : 12 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 2.0s\n",
      " 0.450584 : 12 passes : view2-dbow+dmm 0.0s 3.7s\n",
      " 0.451647 : 12 passes : view2-dbow+dmc 0.0s 3.4s\n",
      "completed pass 12 at alpha 0.011800\n",
      " 0.453773 : 13 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      " 0.442083 : 13 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.8s\n",
      "*0.450584 : 13 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 2.0s\n",
      " 0.450584 : 13 passes : view2-dbow+dmm 0.0s 3.6s\n",
      " 0.451647 : 13 passes : view2-dbow+dmc 0.0s 4.1s\n",
      "completed pass 13 at alpha 0.010600\n",
      " 0.451647 : 14 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.444208 : 14 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 14 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.6s\n",
      " 0.450584 : 14 passes : view2-dbow+dmm 0.0s 4.7s\n",
      " 0.451647 : 14 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 14 at alpha 0.009400\n",
      " 0.452710 : 15 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.500000 : 15 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.8s\n",
      " 0.447396 : 15 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.361702 : 15 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.8s\n",
      "*0.450584 : 15 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      "*0.425532 : 15 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 2.0s\n",
      " 0.450584 : 15 passes : view2-dbow+dmm 0.0s 4.6s\n",
      " 0.382979 : 15 passes : view2-dbow+dmm_inferred 0.0s 3.6s\n",
      " 0.451647 : 15 passes : view2-dbow+dmc 0.0s 4.4s\n",
      " 0.446809 : 15 passes : view2-dbow+dmc_inferred 0.0s 3.4s\n",
      "completed pass 15 at alpha 0.008200\n",
      " 0.451647 : 16 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.446334 : 16 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 16 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.450584 : 16 passes : view2-dbow+dmm 0.0s 4.7s\n",
      " 0.451647 : 16 passes : view2-dbow+dmc 0.0s 4.3s\n",
      "completed pass 16 at alpha 0.007000\n",
      " 0.450584 : 17 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.448459 : 17 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 17 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.450584 : 17 passes : view2-dbow+dmm 0.0s 4.7s\n",
      " 0.451647 : 17 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 17 at alpha 0.005800\n",
      " 0.452710 : 18 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.447396 : 18 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 18 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.450584 : 18 passes : view2-dbow+dmm 0.0s 4.7s\n",
      " 0.451647 : 18 passes : view2-dbow+dmc 0.0s 4.3s\n",
      "completed pass 18 at alpha 0.004600\n",
      " 0.451647 : 19 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.6s 2.3s\n",
      " 0.446334 : 19 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      "*0.450584 : 19 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      " 0.450584 : 19 passes : view2-dbow+dmm 0.0s 4.6s\n",
      " 0.451647 : 19 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "completed pass 19 at alpha 0.003400\n",
      " 0.450584 : 20 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 2.3s\n",
      " 0.425532 : 20 passes : view2-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.8s\n",
      " 0.446334 : 20 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 2.3s\n",
      " 0.457447 : 20 passes : view2-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.8s\n",
      "*0.450584 : 20 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 2.5s\n",
      "*0.414894 : 20 passes : view2-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 2.1s\n",
      " 0.450584 : 20 passes : view2-dbow+dmm 0.0s 4.6s\n",
      " 0.446809 : 20 passes : view2-dbow+dmm_inferred 0.0s 3.6s\n",
      " 0.451647 : 20 passes : view2-dbow+dmc 0.0s 4.4s\n",
      "*0.404255 : 20 passes : view2-dbow+dmc_inferred 0.0s 3.4s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2015-12-10 13:00:35.240954\n"
     ]
    }
   ],
   "source": [
    "print 'Started.'\n",
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "for view in [v1, v2]:\n",
    "    alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "    alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "    print \"======== %s =========\" %view\n",
    "    print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "    for epoch in range(passes):\n",
    "        shuffle(doc_list[view])  # shuffling gets best results\n",
    "\n",
    "        for name, train_model in models_by_name[view].items():\n",
    "            #print name\n",
    "            # train\n",
    "            duration = 'na'\n",
    "            train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "            with elapsed_timer() as elapsed:\n",
    "                train_model.train(doc_list[view])\n",
    "                duration = '%.1f' % elapsed()\n",
    "\n",
    "            # evaluate\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view])\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if err <= best_error[view][name][0]:\n",
    "                best_error[view][name] = (err, alpha)\n",
    "                best_indicator = '*' \n",
    "            print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, err, epoch + 1, view, name, duration, eval_duration))\n",
    "\n",
    "            if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "                eval_duration = ''\n",
    "                with elapsed_timer() as eval_elapsed:\n",
    "                    infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view], infer=True)\n",
    "                eval_duration = '%.1f' % eval_elapsed()\n",
    "                best_indicator = ' '\n",
    "                if infer_err < best_error[view][name + '_inferred'][0]:\n",
    "                    best_error[view][name + '_inferred'] = (infer_err, alpha)\n",
    "                    best_indicator = '*'\n",
    "                print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, infer_err, epoch + 1, view, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "        print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "        alpha -= alpha_delta\n",
    "\n",
    "    print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= view1 ========\n",
      "0.361702 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.020200\n",
      "0.361702 Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.025000\n",
      "0.362380 dbow+dmc 0.002200\n",
      "0.369819 Doc2Vec(dbow,d200,n5,mc5,t4) 0.010600\n",
      "0.382979 dbow+dmc_inferred 0.025000\n",
      "0.391073 dbow+dmm 0.025000\n",
      "0.393617 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.025000\n",
      "0.393617 dbow+dmm_inferred 0.025000\n",
      "0.397450 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.010600\n",
      "0.399575 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.015400\n",
      "========= view2 ========\n",
      "0.319149 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.020200\n",
      "0.351064 dbow+dmc_inferred 0.002200\n",
      "0.404255 dbow+dmm_inferred 0.014200\n",
      "0.414894 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.025000\n",
      "0.425080 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.004600\n",
      "0.425080 dbow+dmc 0.022600\n",
      "0.425532 Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.025000\n",
      "0.428268 dbow+dmm 0.015400\n",
      "0.430393 Doc2Vec(dbow,d200,n5,mc5,t4) 0.002200\n",
      "0.444208 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.007000\n"
     ]
    }
   ],
   "source": [
    "for view in [v1, v2]:\n",
    "    print '========= %s ========' %view\n",
    "    for rate, alpha, name in sorted((rate, alpha, name) for name, (rate, alpha) in best_error[view].items()):\n",
    "        print(\"%f %s %f\" % (rate, name, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentDocument(words=u'successful night @ Tikis\\n', tags=[700], split='train', sentiment=1)\n",
      "\n",
      "SentimentDocument(words=u'Pink Tiki TONIGHT @ Tikis Grill & Bar in Waikiki Beach Hotel, 21 and over. 3 hours FREE Valet Parking, $3 Drink Specials, Come visit us ||Positive||\\n', tags=[700], split='train', sentiment=1)\n"
     ]
    }
   ],
   "source": [
    "# Print some example tweets and their vector reps for both views\n",
    "index = 700\n",
    "print alldocs['view1'][index]\n",
    "tag = alldocs['view1'][index].tags[0]\n",
    "#print '\\n', simple_models['view1'][0].docvecs[tag]\n",
    "\n",
    "print '\\n', alldocs['view2'][index]\n",
    "#print '\\n', simple_models['view2'][0].docvecs[tag]\n",
    "#print '\\n\\n', doc_list['view2'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 265...\n",
      "Doc2Vec(dm/c,d200,n5,w3,mc2,t4):\n",
      " [(265, 0.7614240646362305), (1722, 0.5647834539413452), (1272, 0.5114045739173889)]\n",
      "Doc2Vec(dbow,d200,n5,mc5,t4):\n",
      " [(265, 0.901440441608429), (1272, 0.7241289615631104), (464, 0.7001410722732544)]\n",
      "Doc2Vec(dm/m,d200,n5,w3,mc2,t4):\n",
      " [(265, 0.845321536064148), (464, 0.6873066425323486), (1154, 0.626861035823822)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[v1][0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models[v1]:\n",
    "    inferred_docvec = model.infer_vector(alldocs[v1][doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center(data):\n",
    "    return data - np.mean(data, axis=0)\n",
    "\n",
    "def PLS(X, Y):\n",
    "    cross_cov = np.dot(center(X), center(Y).T)\n",
    "    eigval,eigvec=np.linalg.eig(cross_cov.dot(cross_cov.T))\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def PLS_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    accuracies = np.zeros((len(num_neighb), len(dims)))\n",
    "    # (score, dim, k, PLS_subspace, classifier_object)\n",
    "    best = (0, 0, 0, None, None) \n",
    "    #run pls\n",
    "    eigval, U = PLS(acoustic_train, artic_train)\n",
    "    for j, k in enumerate(num_neighb):\n",
    "        for i, d in enumerate(dims):\n",
    "            U_d = get_top_eigvec(eigval, U, d)\n",
    "            #get projection to pls space\n",
    "            train_proj = np.dot(U_d.T, acoustic_train_cen)\n",
    "            dev_proj = np.dot(U_d.T, acoustic_dev_cen)\n",
    "            # stack with mfcc39\n",
    "            stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "            stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "            \n",
    "            #classify\n",
    "            clf = neighbors.KNeighborsClassifier(k)\n",
    "            clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "            #predictions\n",
    "            score = clf.score(stacked_dev.T, phones_dev)\n",
    "            if score > best[0]:\n",
    "                best = (score, d, k, U_d, clf)\n",
    "            accuracies[j,i] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def CCA(X, Y, regX = 0, regY = 0):\n",
    "    cenX = center(X)\n",
    "    cenY = center(Y)\n",
    "    cross_cov = cenX.dot(cenY.T)\n",
    "    covX = cenX.dot(cenX.T)\n",
    "    covY = cenY.dot(cenY.T)\n",
    "    r_Ix = regX * np.eye(covX.shape[0])\n",
    "    r_Iy = regY * np.eye(covY.shape[0])\n",
    "    A = reduce(np.dot, [ np.linalg.inv(covX + r_Ix), cross_cov, np.linalg.inv(covY + r_Iy), cross_cov.T ])\n",
    "    eigval,eigvec=np.linalg.eig(A)\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def get_top_eigvec(eigval, eigvec, k):\n",
    "    idx=np.argsort(eigval)[-k:][::-1]\n",
    "    #eigval=eigval[idx]\n",
    "    return eigvec[:,idx]\n",
    "\n",
    "def CCA_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    reg = [1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1e1]\n",
    "    accuracies = np.zeros((len(reg), len(reg), len(dims), len(num_neighb)))\n",
    "    # (score, dim, regX, regY, k, CCA_subspace, classifier_object)\n",
    "    best = (0, 0, 0, 0, 0, None, None)\n",
    "    for rx, regX in enumerate(reg):\n",
    "        for ry, regY in enumerate(reg):              \n",
    "            #run cca\n",
    "            eigval, U = CCA(acoustic_train, artic_train, regX, regY)\n",
    "            for i, d in enumerate(dims):\n",
    "                U_d = get_top_eigvec(eigval, U, d)\n",
    "                #get projection to cca space\n",
    "                train_proj = U_d.T.dot(acoustic_train_cen)\n",
    "                dev_proj = U_d.T.dot(acoustic_dev_cen)\n",
    "                # stack with mfcc39\n",
    "                stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "                stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "                #classify\n",
    "                for j, k in enumerate(num_neighb):\n",
    "                    clf = neighbors.KNeighborsClassifier(k)\n",
    "                    clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "                    #predictions\n",
    "                    score = clf.score(stacked_dev.T, phones_dev)\n",
    "                    if score > best[0]:\n",
    "                        best = (score, d, regX, regY, k, U_d, clf)\n",
    "                    accuracies[rx, ry, i, j] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def plot_pc2(data, eigvec, phones_data):\n",
    "    #project to top 2 princ. comp.\n",
    "    data_proj = np.dot(np.transpose(eigvec), data)\n",
    "    data_proj_labels=[data_proj[:,np.where(phones_data==lbl)] for lbl in labels_dict.values()]\n",
    "    #Plot\n",
    "    cmap = plt.get_cmap('jet_r')\n",
    "    N=len(labels)\n",
    "    colors = [cmap(float(i)/N) for i in np.linspace(5.0, 0, N)]\n",
    "    plt.figure(figsize=(7,7))\n",
    "    #plt.subplot(2,1,1)\n",
    "    for i in range(N):\n",
    "        plt.scatter(data_proj_labels[i][0,:], data_proj_labels[i][1,:] ,c=colors[i], marker='+', label=labels[i]);\n",
    "    #plt.legend(plots,labels)\n",
    "    plt.legend(loc=3)\n",
    "    #plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select the best performing word2vec model\n",
    "best_model_name = 'dbow+dmc'\n",
    "best_alpha = 0.0022000\n",
    "best_model = { v1 : models_by_name[v1][best_model_name],\n",
    "              v2 : models_by_name[v2][best_model_name] }\n",
    "# Train best model\n",
    "shuffle(doc_list[view])\n",
    "for view in [v1, v2]:\n",
    "    best_model[v1].alpha, best_model[v1].min_alpha = best_alpha, best_alpha\n",
    "    best_model[v1].train(doc_list[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # DO CCA on the training docvecs\n",
    "# X = view 1, Y = view 2 : [num_samples x word_vec_size]\n",
    "target_sentiments, X, Y = zip(*[(doc.sentiment, best_model[v1].docvecs[doc.tags[0]], \\\n",
    "                             best_model[v2].docvecs[doc.tags[0]]) for doc in train_docs[v1]])\n",
    "X = np.asarray(X).T\n",
    "Y = np.asarray(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(cca_eigval, cca_eigvec) = CCA(X, Y)\n",
    "#print np.shape(X), np.shape(Y)\n",
    "#print np.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 50)\n",
      "(2196, 50)\n",
      "(2196, 450)\n"
     ]
    }
   ],
   "source": [
    "#get top k eigvec, project training data and stack with original word vectors\n",
    "top_k_eigv = get_top_eigvec(cca_eigval, cca_eigvec, 50)\n",
    "print np.shape(top_k_eigv)\n",
    "X_proj = X.dot(top_k_eigv)\n",
    "print np.shape(X_proj)\n",
    "stacked_vec = np.append(X, X_proj, axis=1)\n",
    "print np.shape(stacked_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# project test data to cca directions and stack\n",
    "test = [best_model[v1].docvecs[doc.tags[0]] for doc in test_docs[v1]]\n",
    "test = np.asarray(test)\n",
    "#print np.shape(test)\n",
    "test_proj = test.dot(top_k_eigv)\n",
    "stacked_test = np.append(test, test_proj, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.472901168969\n"
     ]
    }
   ],
   "source": [
    "predictor = svm_predictor_from_data(target_sentiments, stacked_vec)\n",
    "\n",
    "# predict & evaluate\n",
    "test_predictions = predictor.predict(stacked_test)\n",
    "predicted = np.rint(test_predictions)\n",
    "expected = [doc.sentiment for doc in test_docs[v1]]\n",
    "\"\"\"if not infer:\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (predictor, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\"\"\"\n",
    "#ipdb.set_trace()\n",
    "corrects = sum(expected == predicted)\n",
    "errors = len(test_predictions) - corrects\n",
    "print float(errors) / len(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
