{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/view1_clean not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Split the all_views, get one file(sentiment, tweet) for each view\n",
    "#Clean each view\n",
    "#Run this for each view.\n",
    "norm() {\n",
    "    fn=$1\n",
    "    if [ ! -f \"$fn\" ]\n",
    "    then\n",
    "        echo \"File: $fn not found\"\n",
    "        exit\n",
    "    fi\n",
    "    #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
    "    function normalize_text {\n",
    "        awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
    "        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
    "        -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
    "    }\n",
    "    export LC_ALL=C\n",
    "    normalize_text \"$fn\"\n",
    "    wc -l $fn\n",
    "    mv \"$fn\" \"$fn-norm\"\n",
    "}\n",
    "norm \"data/view1_clean\" #file name is\n",
    "norm \"data/view2_clean\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "tw_view_1 = 'data/view1_clean-norm'\n",
    "tw_view_2 = 'data/view2_clean-norm'\n",
    "assert os.path.isfile(tw_view_1), tw_view_1 + \" unavailable\"\n",
    "assert os.path.isfile(tw_view_2), tw_view_2 + \" unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3805 docs: 2664 train-sentiment, 1141 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple, defaultdict as dd\n",
    "\n",
    "#sentiment = {'positive':1, 'negative':-1} #, 'neutral':2}\n",
    "sentiment_dict = {'4':1, '0':-1} #- new data 0,4\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = dd(list)  # will hold all docs in original order - dictionary, keys = [v1, v2]\n",
    "v1 = 'view1'\n",
    "v2 = 'view2'\n",
    "#tw_sentiment_dict = {}\n",
    "#print total_num, train_test_shuffle\n",
    "all_v2_words = []\n",
    "with open(tw_view_2) as allview2:\n",
    "        all_v2_words = allview2.readlines()\n",
    "total_num = len(all_v2_words)\n",
    "#split train/test\n",
    "train_num = total_num *  7 / 10 # 70% train/test 1 - 10\n",
    "train_test_shuffle = np.arange(total_num)\n",
    "np.random.shuffle(train_test_shuffle)\n",
    "with open(tw_view_1) as allview1:\n",
    "    #for line_no, (v1, v2) in enumerate(zip(allview1, allview2)):\n",
    "    for line_no, line in enumerate(allview1):\n",
    "        tokens = gensim.utils.to_unicode(line).split('\\t')\n",
    "        if len(tokens) != 2:\n",
    "            print line\n",
    "            raise Exception()\n",
    "        sentiment = sentiment_dict[tokens[0]]\n",
    "        #if tw_id not in tw_sentiment_dict.keys():\n",
    "        #    continue\n",
    "        words = tokens[1]\n",
    "        split = 'train' if train_test_shuffle[line_no] <= train_num else 'test'\n",
    "        #sentiment = tw_sentiment_dict[tw_id]\n",
    "        v2_words = gensim.utils.to_unicode(all_v2_words[line_no]).split('\\t')[1]\n",
    "        \n",
    "        alldocs[v1].append(SentimentDocument(words, [line_no], split, sentiment))\n",
    "        alldocs[v2].append(SentimentDocument(v2_words, [line_no], split, sentiment))\n",
    "train_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'train'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'train']\n",
    "}\n",
    "test_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'test'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'test']\n",
    "}\n",
    "doc_list = { v1: alldocs[v1][:], v2: alldocs[v2][:] }  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list[v1]), len(train_docs[v1]), len(test_docs[v1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view1 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view1 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view1 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view2 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "model_size = 200\n",
    "simple_models , models_by_name = {}, {} \n",
    "for view in [v1, v2]:\n",
    "    simple_models[view] = [\n",
    "        # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "        Doc2Vec(dm=1, dm_concat=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "        # PV-DBOW \n",
    "        Doc2Vec(dm=0, size=model_size, negative=5, hs=0, min_count=5, workers=cores),\n",
    "        # PV-DM w/average\n",
    "        Doc2Vec(dm=1, dm_mean=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    ]\n",
    "\n",
    "    # speed setup by sharing results of 1st model's vocabulary scan\n",
    "    simple_models[view][0].build_vocab(alldocs[view])  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "    print view, simple_models[view][0]\n",
    "    for model in simple_models[view][1:]:\n",
    "        model.reset_from(simple_models[view][0])\n",
    "        print view, model\n",
    "\n",
    "    models_by_name[view] = OrderedDict((str(model), model) for model in simple_models[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "for view in [v1, v2]:\n",
    "    models_by_name[view]['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][2]])\n",
    "    models_by_name[view]['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][0]])\n",
    "#print models_by_name['dbow+dmm'], models_by_name['dbow+dmc'] \n",
    "#del models_by_name['dbow+dmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn import svm, metrics, neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "import ipdb\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor(train_targets, train_regressors):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_regressors, train_targets)\n",
    "    return lr\n",
    "\n",
    "def svm_predictor(train_targets, train_regressors):\n",
    "    svc = svm.SVC(kernel='rbf', degree=5, gamma=1e-1)\n",
    "    svc.fit(train_regressors, train_targets)\n",
    "    return svc\n",
    "\n",
    "    \"\"\"expected = svm_y_test\n",
    "    predicted = svc.predict(svm_x_test)\n",
    "\n",
    "    #print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "    #      % (svc, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    \"\"\"\n",
    "def rf_predictor(train_targets, train_regressors):\n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc.fit(train_regressors, train_targets)\n",
    "    return rfc\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    predictor = predictor_alg(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_data]\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_data]\n",
    "    \"\"\"if not infer:\n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "              % (predictor, metrics.classification_report(expected, predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    corrects = sum(expected == predicted)\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "def center(data):\n",
    "    return data - np.mean(data, axis=0)\n",
    "\n",
    "def PLS(X, Y):\n",
    "    cross_cov = np.dot(center(X), center(Y).T)\n",
    "    eigval,eigvec=np.linalg.eig(cross_cov.dot(cross_cov.T))\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def PLS_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    accuracies = np.zeros((len(num_neighb), len(dims)))\n",
    "    # (score, dim, k, PLS_subspace, classifier_object)\n",
    "    best = (0, 0, 0, None, None) \n",
    "    #run pls\n",
    "    eigval, U = PLS(acoustic_train, artic_train)\n",
    "    for j, k in enumerate(num_neighb):\n",
    "        for i, d in enumerate(dims):\n",
    "            U_d = get_top_eigvec(eigval, U, d)\n",
    "            #get projection to pls space\n",
    "            train_proj = np.dot(U_d.T, acoustic_train_cen)\n",
    "            dev_proj = np.dot(U_d.T, acoustic_dev_cen)\n",
    "            # stack with mfcc39\n",
    "            stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "            stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "            \n",
    "            #classify\n",
    "            clf = neighbors.KNeighborsClassifier(k)\n",
    "            clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "            #predictions\n",
    "            score = clf.score(stacked_dev.T, phones_dev)\n",
    "            if score > best[0]:\n",
    "                best = (score, d, k, U_d, clf)\n",
    "            accuracies[j,i] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def CCA(X, Y, regX = 0, regY = 0):\n",
    "    cenX = center(X)\n",
    "    cenY = center(Y)\n",
    "    cross_cov = cenX.dot(cenY.T)\n",
    "    covX = cenX.dot(cenX.T)\n",
    "    covY = cenY.dot(cenY.T)\n",
    "    r_Ix = regX * np.eye(covX.shape[0])\n",
    "    r_Iy = regY * np.eye(covY.shape[0])\n",
    "    A = reduce(np.dot, [ np.linalg.inv(covX + r_Ix), cross_cov, np.linalg.inv(covY + r_Iy), cross_cov.T ])\n",
    "    eigval,eigvec=np.linalg.eig(A)\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def get_top_eigvec(eigval, eigvec, k):\n",
    "    idx=np.argsort(eigval)[-k:][::-1]\n",
    "    #eigval=eigval[idx]\n",
    "    return eigvec[:,idx]\n",
    "\n",
    "def CCA_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    reg = [1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1e1]\n",
    "    accuracies = np.zeros((len(reg), len(reg), len(dims), len(num_neighb)))\n",
    "    # (score, dim, regX, regY, k, CCA_subspace, classifier_object)\n",
    "    best = (0, 0, 0, 0, 0, None, None)\n",
    "    for rx, regX in enumerate(reg):\n",
    "        for ry, regY in enumerate(reg):              \n",
    "            #run cca\n",
    "            eigval, U = CCA(acoustic_train, artic_train, regX, regY)\n",
    "            for i, d in enumerate(dims):\n",
    "                U_d = get_top_eigvec(eigval, U, d)\n",
    "                #get projection to cca space\n",
    "                train_proj = U_d.T.dot(acoustic_train_cen)\n",
    "                dev_proj = U_d.T.dot(acoustic_dev_cen)\n",
    "                # stack with mfcc39\n",
    "                stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "                stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "                #classify\n",
    "                for j, k in enumerate(num_neighb):\n",
    "                    clf = neighbors.KNeighborsClassifier(k)\n",
    "                    clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "                    #predictions\n",
    "                    score = clf.score(stacked_dev.T, phones_dev)\n",
    "                    if score > best[0]:\n",
    "                        best = (score, d, regX, regY, k, U_d, clf)\n",
    "                    accuracies[rx, ry, i, j] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def plot_pc2(data, eigvec, phones_data):\n",
    "    #project to top 2 princ. comp.\n",
    "    data_proj = np.dot(np.transpose(eigvec), data)\n",
    "    data_proj_labels=[data_proj[:,np.where(phones_data==lbl)] for lbl in labels_dict.values()]\n",
    "    #Plot\n",
    "    cmap = plt.get_cmap('jet_r')\n",
    "    N=len(labels)\n",
    "    colors = [cmap(float(i)/N) for i in np.linspace(5.0, 0, N)]\n",
    "    plt.figure(figsize=(7,7))\n",
    "    #plt.subplot(2,1,1)\n",
    "    for i in range(N):\n",
    "        plt.scatter(data_proj_labels[i][0,:], data_proj_labels[i][1,:] ,c=colors[i], marker='+', label=labels[i]);\n",
    "    #plt.legend(plots,labels)\n",
    "    plt.legend(loc=3)\n",
    "    #plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "best_error = dd(lambda: dd(lambda :(1.0, 0.0))) # { view: { model_name : (error_rate, alpha) } } ,to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started.\n",
      "======== view1 =========\n",
      "START 2015-12-11 11:37:26.391044\n",
      "END 2015-12-11 11:37:43.087232\n",
      "======== view2 =========\n",
      "START 2015-12-11 11:37:43.087662\n",
      "END 2015-12-11 11:37:59.801377\n"
     ]
    }
   ],
   "source": [
    "predictor_alg = logistic_predictor\n",
    "#predictor_alg = svm_predictor\n",
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "print 'Started.'\n",
    "for view in [v1, v2]:\n",
    "    alpha, min_alpha, passes = (0.025, 0.001, 10)\n",
    "    alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "    print \"======== %s =========\" %view\n",
    "    print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "    for epoch in range(passes):\n",
    "        shuffle(doc_list[view])  # shuffling gets best results\n",
    "\n",
    "        for name, train_model in models_by_name[view].items():\n",
    "            #print name\n",
    "            # train\n",
    "            duration = 'na'\n",
    "            train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "            with elapsed_timer() as elapsed:\n",
    "                train_model.train(doc_list[view])\n",
    "                duration = '%.1f' % elapsed()\n",
    "\n",
    "            # evaluate\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view])\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if err < best_error[view][name][0]:\n",
    "                best_error[view][name] = (err, alpha)\n",
    "                best_indicator = '*' \n",
    "            #print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, err, epoch + 1, view, name, duration, eval_duration))\n",
    "\n",
    "            \"\"\"if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "                eval_duration = ''\n",
    "                with elapsed_timer() as eval_elapsed:\n",
    "                    infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view], infer=True)\n",
    "                eval_duration = '%.1f' % eval_elapsed()\n",
    "                best_indicator = ' '\n",
    "                if infer_err < best_error[view][name + '_inferred'][0]:\n",
    "                    best_error[view][name + '_inferred'] = (infer_err, alpha)\n",
    "                    best_indicator = '*'\n",
    "                print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, infer_err, epoch + 1, view, name + '_inferred', duration, eval_duration))\n",
    "\"\"\"\n",
    "        #print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "        alpha -= alpha_delta\n",
    "\n",
    "    print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= view1 ========\n",
      "0.374233 dbow+dmc 0.015400\n",
      "0.378615 dbow+dmm 0.008200\n",
      "0.383874 Doc2Vec(dbow,d200,n5,mc5,t4) 0.005800\n",
      "0.425066 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.017800\n",
      "0.425942 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.008200\n",
      "========= view2 ========\n",
      "0.432077 dbow+dmc 0.017800\n",
      "0.437336 Doc2Vec(dbow,d200,n5,mc5,t4) 0.015400\n",
      "0.441718 dbow+dmm 0.025000\n",
      "0.444347 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.008200\n",
      "0.446976 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.015400\n"
     ]
    }
   ],
   "source": [
    "for view in [v1, v2]:\n",
    "    print '========= %s ========' %view\n",
    "    for rate, alpha, name in sorted((rate, alpha, name) for name, (rate, alpha) in best_error[view].items()):\n",
    "        print(\"%f %s %f\" % (rate, name, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 478...\n",
      "SentimentDocument(words=u'ummm....waching tv and drinking PEPSI!!! yummey\\n', tags=[478], split='train', sentiment=1)\n",
      "\n",
      "SentimentDocument(words=u'ummm.... i just got done sk8ing and waching charlie the unicorn 1,2,3 ||Positive||.\\n', tags=[478], split='train', sentiment=1)\n",
      "Doc2Vec(dm/c,d200,n5,w3,mc2,t4):\n",
      " [(501, 0.8668317198753357), (478, 0.8615103363990784), (182, 0.7848052978515625)]\n",
      "Doc2Vec(dbow,d200,n5,mc5,t4):\n",
      " [(478, 0.9324709177017212), (2938, 0.7359375953674316), (1364, 0.7335247993469238)]\n",
      "Doc2Vec(dm/m,d200,n5,w3,mc2,t4):\n",
      " [(478, 0.7126620411872864), (2938, 0.6825379133224487), (1374, 0.6084405183792114)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[v1][0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "# Print example tweet and vector reps for both views\n",
    "print alldocs['view1'][doc_id]\n",
    "#tag = alldocs['view1'][doc_id].tags[0]\n",
    "#print '\\n', simple_models['view1'][0].docvecs[tag]\n",
    "\n",
    "print '\\n', alldocs['view2'][doc_id]\n",
    "#print '\\n', simple_models['view2'][0].docvecs[tag]\n",
    "#print '\\n\\n', doc_list['view2'][:10]\n",
    "for model in simple_models[v1]:\n",
    "    inferred_docvec = model.infer_vector(alldocs[v1][doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbow+dmc\n",
      "0.0154\n"
     ]
    }
   ],
   "source": [
    "#Select the best performing word2vec model\n",
    "_, best_alpha, best_model_name = min(((rate, alpha, name) \\\n",
    "                                           for name, (rate, alpha) in best_error[v1].items()), key=lambda b: b[0])\n",
    "print best_model_name \n",
    "print best_alpha\n",
    "best_model = { v1 : models_by_name[v1][best_model_name],\n",
    "              v2 : models_by_name[v2][best_model_name] }\n",
    "# Train best model\n",
    "shuffle(doc_list[view])\n",
    "for view in [v1, v2]:\n",
    "    best_model[view].alpha, best_model[view].min_alpha = best_alpha, best_alpha\n",
    "    best_model[view].train(doc_list[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # DO CCA on the training docvecs\n",
    "# X = view 1, Y = view 2 : [word_vec_size x num_samples]\n",
    "target_sentiments, X, Y = zip(*[(doc.sentiment, best_model[v1].docvecs[doc.tags[0]], \\\n",
    "                             best_model[v2].docvecs[doc.tags[0]]) for doc in train_docs[v1]])\n",
    "X = np.asarray(X).T\n",
    "Y = np.asarray(Y).T\n",
    "#test docs - view1\n",
    "test = [best_model[v1].docvecs[doc.tags[0]] for doc in test_docs[v1]]\n",
    "test = np.asarray(test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(cca_eigval, cca_eigvec) = CCA(X, Y)\n",
    "#print np.shape(X), np.shape(Y)\n",
    "#print np.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[276 241]\n",
      " [193 431]]\n",
      "0.38036809816\n"
     ]
    }
   ],
   "source": [
    "predictor = predictor_alg(target_sentiments, X.T)\n",
    "\n",
    "# predict & evaluate\n",
    "test_predictions = predictor.predict(test.T)\n",
    "predicted = np.rint(test_predictions)\n",
    "expected = [doc.sentiment for doc in test_docs[v1]]\n",
    "#print(\"Classification report for %s:\\n%s\\n\" % (predictor, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "#ipdb.set_trace()\n",
    "errors = len(test_predictions) - sum(expected == predicted)\n",
    "err_orig = float(errors) / len(expected)\n",
    "print err_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get top k eigvec, project training data and stack with original word vectors\n",
    "err_cca = []\n",
    "step = 5\n",
    "num_dir_ranges = range(step, step+200, step)\n",
    "for num_dir in num_dir_ranges:\n",
    "    top_k_eigv = get_top_eigvec(cca_eigval, cca_eigvec, num_dir)\n",
    "    #print np.shape(top_k_eigv)\n",
    "    X_proj = top_k_eigv.T.dot(X)\n",
    "    #print np.shape(X_proj)\n",
    "    stacked_vec = np.append(X, X_proj, axis=0)\n",
    "    #print np.shape(stacked_vec)\n",
    "\n",
    "    # project test data to cca directions and stack\n",
    "    #print np.shape(test)\n",
    "    test_proj = top_k_eigv.T.dot(test)\n",
    "    stacked_test = np.append(test, test_proj, axis=0)\n",
    "    #print np.shape(stacked_test)\n",
    "    #print np.shape(target_sentiments)\n",
    "\n",
    "    predictor = predictor_alg(target_sentiments, stacked_vec.T)\n",
    "\n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(stacked_test.T)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_docs[v1]]\n",
    "    #print(\"Classification report for %s:\\n%s\\n\" % (predictor, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    #ipdb.set_trace()\n",
    "    errors = len(test_predictions) - sum(expected == predicted)\n",
    "    err_cca.append(float(errors) / len(expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEKCAYAAAAmfuNnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1144c5810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_dir_ranges, [err_orig]*len(num_dir_ranges), 'r--', num_dir_ranges, err_cca, 'b*')\n",
    "plt.ylabel('Mis-classification rate')\n",
    "plt.xlabel('Number of CCA directions stacked with original sentence vectors')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
