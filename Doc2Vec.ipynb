{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from timeit import default_timer\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    print 'I\\'m here.' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    print 'I\\'m done.' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here.2015-12-09 12:47:26\n",
      "1.0\n",
      "I'm done.2015-12-09 12:47:31\n"
     ]
    }
   ],
   "source": [
    "with elapsed_timer() as elapsed:\n",
    "    time.sleep(1)\n",
    "    duration = '%.1f' % elapsed()\n",
    "    print duration\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3137 data/view1_clean\n",
      "    3137 data/view2_clean\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Split the all_views, get one file(sentiment, tweet) for each view\n",
    "#Clean each view\n",
    "#Run this for each view.\n",
    "norm() {\n",
    "    fn=$1\n",
    "    if [ ! -f \"$fn\" ]\n",
    "    then\n",
    "        echo \"File: $fn not found\"\n",
    "        exit\n",
    "    fi\n",
    "    #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
    "    function normalize_text {\n",
    "        awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
    "        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
    "        -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
    "    }\n",
    "    export LC_ALL=C\n",
    "    normalize_text \"$fn\"\n",
    "    wc -l $fn\n",
    "    mv \"$fn\" \"$fn-norm\"\n",
    "}\n",
    "norm \"data/view1_clean\" #file name is\n",
    "norm \"data/view2_clean\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "tw_view_1 = 'data/view1_clean-norm'\n",
    "tw_view_2 = 'data/view2_clean-norm'\n",
    "assert os.path.isfile(tw_view_1), tw_view_1 + \" unavailable\"\n",
    "assert os.path.isfile(tw_view_2), tw_view_2 + \" unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3137 docs: 2196 train-sentiment, 941 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple, defaultdict as dd\n",
    "\n",
    "#sentiment = {'positive':1, 'negative':-1} #, 'neutral':2}\n",
    "sentiment_dict = {'4':1, '0':-1} #- new data 0,4\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = dd(list)  # will hold all docs in original order - dictionary, keys = [v1, v2]\n",
    "v1 = 'view1'\n",
    "v2 = 'view2'\n",
    "#tw_sentiment_dict = {}\n",
    "#print total_num, train_test_shuffle\n",
    "all_v2_words = []\n",
    "with open(tw_view_2) as allview2:\n",
    "        all_v2_words = allview2.readlines()\n",
    "total_num = len(all_v2_words)\n",
    "#split train/test\n",
    "train_num = total_num *  7 / 10 # 70% train/test\n",
    "train_test_shuffle = np.arange(total_num)\n",
    "np.random.shuffle(train_test_shuffle)\n",
    "with open(tw_view_1) as allview1:\n",
    "    #for line_no, (v1, v2) in enumerate(zip(allview1, allview2)):\n",
    "    for line_no, line in enumerate(allview1):\n",
    "        tokens = gensim.utils.to_unicode(line).split('\\t')\n",
    "        if len(tokens) != 2:\n",
    "            print line\n",
    "            raise Exception()\n",
    "        sentiment = sentiment_dict[tokens[0]]\n",
    "        #if tw_id not in tw_sentiment_dict.keys():\n",
    "        #    continue\n",
    "        words = tokens[1]\n",
    "        split = 'train' if train_test_shuffle[line_no] <= train_num else 'test'\n",
    "        #sentiment = tw_sentiment_dict[tw_id]\n",
    "        v2_words = gensim.utils.to_unicode(all_v2_words[line_no]).split('\\t')[1]\n",
    "        \n",
    "        alldocs[v1].append(SentimentDocument(words, [line_no], split, sentiment))\n",
    "        alldocs[v2].append(SentimentDocument(v2_words, [line_no], split, sentiment))\n",
    "train_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'train'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'train']\n",
    "}\n",
    "test_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'test'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'test']\n",
    "}\n",
    "doc_list = { v1: alldocs[v1][:], v2: alldocs[v2][:] }  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list[v1]), len(train_docs[v1]), len(test_docs[v1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view1 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view1 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view1 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view2 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "model_size = 200\n",
    "simple_models , models_by_name = {}, {} \n",
    "for view in [v1, v2]:\n",
    "    simple_models[view] = [\n",
    "        # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "        Doc2Vec(dm=1, dm_concat=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "        # PV-DBOW \n",
    "        Doc2Vec(dm=0, size=model_size, negative=5, hs=0, min_count=5, workers=cores),\n",
    "        # PV-DM w/average\n",
    "        Doc2Vec(dm=1, dm_mean=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    ]\n",
    "\n",
    "    # speed setup by sharing results of 1st model's vocabulary scan\n",
    "    simple_models[view][0].build_vocab(alldocs[view])  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "    print view, simple_models[view][0]\n",
    "    for model in simple_models[view][1:]:\n",
    "        model.reset_from(simple_models[view][0])\n",
    "        print view, model\n",
    "\n",
    "    models_by_name[view] = OrderedDict((str(model), model) for model in simple_models[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "for view in [v1, v2]:\n",
    "    models_by_name[view]['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][2]])\n",
    "    models_by_name[view]['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][0]])\n",
    "#print models_by_name['dbow+dmm'], models_by_name['dbow+dmc'] \n",
    "#del models_by_name['dbow+dmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn import svm, metrics, neighbors\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "import ipdb\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    #ipdb.set_trace()\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def svm_predictor_from_data(train_targets, train_regressors):\n",
    "    svc = svm.SVC(kernel='rbf', degree=5, gamma=1e-3)\n",
    "    svc.fit(train_regressors, train_targets)\n",
    "    return svc\n",
    "\n",
    "    \"\"\"expected = svm_y_test\n",
    "    predicted = svc.predict(svm_x_test)\n",
    "\n",
    "    #print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "    #      % (svc, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    \"\"\"\n",
    "    \n",
    "def knn_predictor_from_data(train_targets, train_regressors):\n",
    "    k = 5\n",
    "    clf = neighbors.KNeighborsClassifier(k)\n",
    "    clf.fit(train_regressors, train_targets)\n",
    "    return clf\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    #train_regressors = sm.add_constant(train_regressors)\n",
    "    #predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "    predictor = svm_predictor_from_data(train_targets, train_regressors)\n",
    "    #predictor = knn_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_data]\n",
    "    #test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_data]\n",
    "    \"\"\"if not infer:\n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "              % (predictor, metrics.classification_report(expected, predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    corrects = sum(expected == predicted)\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "best_error = dd(lambda: dd(lambda :(1.0, 0.0)))  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started.\n",
      "======== view1 =========\n",
      "START 2015-12-10 11:48:17.333701\n",
      "*0.448459 : 1 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.8s\n",
      "*0.500000 : 1 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.5s\n",
      "*0.448459 : 1 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.7s\n",
      "*0.351064 : 1 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.4s\n",
      "*0.447396 : 1 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 1.8s\n",
      "*0.468085 : 1 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.3s 1.3s\n",
      "*0.428268 : 1 passes : view1-dbow+dmm 0.0s 3.2s\n",
      "*0.531915 : 1 passes : view1-dbow+dmm_inferred 0.0s 2.5s\n",
      "*0.448459 : 1 passes : view1-dbow+dmc 0.0s 3.2s\n",
      "*0.489362 : 1 passes : view1-dbow+dmc_inferred 0.0s 2.5s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.448459 : 2 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.6s\n",
      "*0.448459 : 2 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.6s\n",
      "*0.447396 : 2 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 1.7s\n",
      " 0.429330 : 2 passes : view1-dbow+dmm 0.0s 3.3s\n",
      "*0.448459 : 2 passes : view1-dbow+dmc 0.0s 3.2s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.448459 : 3 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      "*0.448459 : 3 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.6s\n",
      "*0.447396 : 3 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 1.6s\n",
      "*0.426142 : 3 passes : view1-dbow+dmm 0.0s 3.1s\n",
      "*0.448459 : 3 passes : view1-dbow+dmc 0.0s 3.1s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.448459 : 4 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      "*0.448459 : 4 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.6s\n",
      "*0.444208 : 4 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.9s\n",
      "*0.421892 : 4 passes : view1-dbow+dmm 0.0s 3.7s\n",
      "*0.448459 : 4 passes : view1-dbow+dmc 0.0s 3.3s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.448459 : 5 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      " 0.500000 : 5 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4)_inferred 0.5s 1.4s\n",
      "*0.448459 : 5 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.7s\n",
      " 0.382979 : 5 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4)_inferred 0.2s 1.3s\n",
      "*0.444208 : 5 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.2s 1.7s\n",
      " 0.531915 : 5 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4)_inferred 0.2s 1.3s\n",
      " 0.427205 : 5 passes : view1-dbow+dmm 0.0s 3.3s\n",
      "*0.500000 : 5 passes : view1-dbow+dmm_inferred 0.0s 2.6s\n",
      "*0.448459 : 5 passes : view1-dbow+dmc 0.0s 3.3s\n",
      "*0.393617 : 5 passes : view1-dbow+dmc_inferred 0.0s 2.6s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.448459 : 6 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      "*0.448459 : 6 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.7s\n",
      "*0.441020 : 6 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 1.7s\n",
      " 0.427205 : 6 passes : view1-dbow+dmm 0.0s 3.3s\n",
      "*0.448459 : 6 passes : view1-dbow+dmc 0.0s 3.3s\n",
      "completed pass 6 at alpha 0.019000\n",
      "*0.448459 : 7 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.4s 1.8s\n",
      "*0.448459 : 7 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.7s\n",
      " 0.442083 : 7 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 1.7s\n",
      "*0.419766 : 7 passes : view1-dbow+dmm 0.0s 3.3s\n",
      "*0.448459 : 7 passes : view1-dbow+dmc 0.0s 3.3s\n",
      "completed pass 7 at alpha 0.017800\n",
      "*0.448459 : 8 passes : view1-Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.5s 1.7s\n",
      "*0.448459 : 8 passes : view1-Doc2Vec(dbow,d200,n5,mc5,t4) 0.2s 1.7s\n",
      "*0.437832 : 8 passes : view1-Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.3s 1.7s\n",
      "*0.416578 : 8 passes : view1-dbow+dmm 0.0s 3.3s"
     ]
    }
   ],
   "source": [
    "print 'Started.'\n",
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "for view in [v1, v2]:\n",
    "    alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "    alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "    print \"======== %s =========\" %view\n",
    "    print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "    for epoch in range(passes):\n",
    "        shuffle(doc_list[view])  # shuffling gets best results\n",
    "\n",
    "        for name, train_model in models_by_name[view].items():\n",
    "            #print name\n",
    "            # train\n",
    "            duration = 'na'\n",
    "            train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "            with elapsed_timer() as elapsed:\n",
    "                train_model.train(doc_list[view])\n",
    "                duration = '%.1f' % elapsed()\n",
    "\n",
    "            # evaluate\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view])\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if err <= best_error[view][name][0]:\n",
    "                best_error[view][name] = (err, alpha)\n",
    "                best_indicator = '*' \n",
    "            print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, err, epoch + 1, view, name, duration, eval_duration))\n",
    "\n",
    "            if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "                eval_duration = ''\n",
    "                with elapsed_timer() as eval_elapsed:\n",
    "                    infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view], infer=True)\n",
    "                eval_duration = '%.1f' % eval_elapsed()\n",
    "                best_indicator = ' '\n",
    "                if infer_err < best_error[view][name + '_inferred'][0]:\n",
    "                    best_error[view][name + '_inferred'] = (infer_err, alpha)\n",
    "                    best_indicator = '*'\n",
    "                print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, infer_err, epoch + 1, view, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "        print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "        alpha -= alpha_delta\n",
    "\n",
    "    print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for view in [v1, v2]:\n",
    "    print '========= %s ========' %view\n",
    "    for rate, alpha, name in sorted((rate, alpha, name) for name, (rate, alpha) in best_error[view].items()):\n",
    "        print(\"%f %s %f\" % (rate, name, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentDocument(words=u\"||T|| Can't wait to see you in 4 sleeps and 3 days!\\n\", tags=[70], split='train', sentiment=1)\n",
      "\n",
      "[ -4.25896272e-02  -7.51020536e-02   4.90097255e-02   1.93141222e-01\n",
      "  -3.93390395e-02  -2.54712440e-02  -1.70356892e-02  -1.23695590e-01\n",
      "   1.25351757e-01  -2.56510638e-03  -2.95490623e-02  -1.99460816e-02\n",
      "  -4.66628447e-02  -9.13957953e-02  -9.39208269e-02   4.74906079e-02\n",
      "  -3.71774212e-02   5.00000305e-02  -5.97346341e-03  -2.45082304e-02\n",
      "   2.24953853e-02   1.57746542e-02   9.52238142e-02  -1.06800541e-01\n",
      "  -1.63270339e-01   8.37060958e-02   3.07731070e-02   4.16026153e-02\n",
      "  -5.64083755e-02  -1.10432275e-01  -6.92696944e-02  -1.12972274e-01\n",
      "   1.35694817e-01   1.09122008e-01  -9.25321039e-03   2.34711356e-02\n",
      "   1.41892079e-02   1.17532341e-02   1.84597040e-04   1.59151733e-01\n",
      "  -5.66027574e-02   1.16361395e-01   3.45780104e-02  -2.97031198e-02\n",
      "  -1.41879460e-02  -9.31217298e-02   2.26680897e-02  -9.47654545e-02\n",
      "   1.54242486e-01  -8.31641927e-02   2.46877559e-02   8.99527296e-02\n",
      "  -2.03056000e-02  -9.95390564e-02  -1.75568592e-02  -4.65108454e-02\n",
      "  -1.36482090e-01  -3.25278342e-02   7.53735676e-02  -1.84322223e-02\n",
      "  -5.07418104e-02  -1.42313000e-02  -1.28538562e-02  -1.24910340e-01\n",
      "  -4.59528062e-03   6.29970208e-02  -1.80861931e-02  -4.20193449e-02\n",
      "   1.88786581e-01   4.25177105e-02  -5.84697872e-02   4.45897095e-02\n",
      "  -9.82199609e-02  -2.10965350e-02   9.49438289e-02  -2.06851438e-02\n",
      "   7.47066438e-02  -5.71511984e-02   4.20829505e-02  -1.10820718e-01\n",
      "  -4.88598756e-02   2.60441490e-02  -9.78244767e-02  -2.65070405e-02\n",
      "   3.10990717e-02   2.11501550e-02  -2.11132824e-01   6.23009130e-02\n",
      "   6.11272864e-02   4.38836478e-02   1.04401521e-01   1.19666196e-01\n",
      "  -8.54512602e-02  -4.65351492e-02   1.02009699e-01   1.68364728e-03\n",
      "   4.84823138e-02   6.31593019e-02   6.11607023e-02   3.95427793e-02]\n",
      "\n",
      "SentimentDocument(words=u\"Yay, tax filed online. Not as bad as I thought since I don't have many income sources nor deductions. Getting a small refund.\\n\", tags=[70], split='train', sentiment=1)\n",
      "\n",
      "[-0.05294529  0.0475913  -0.03681004 -0.04810753  0.07207031  0.03519675\n",
      "  0.10511114 -0.0377118   0.01197412 -0.11577449 -0.04173545 -0.05040118\n",
      "  0.10182869  0.02441678 -0.09747159  0.02818874  0.0034687  -0.08173661\n",
      "  0.00883844  0.08614036  0.05118311 -0.00353917 -0.05438897 -0.0097527\n",
      "  0.10717604  0.03854452  0.05544636  0.09926452 -0.0765205  -0.02271572\n",
      "  0.05412279  0.11844441  0.09757864  0.04779519  0.03256194 -0.12943378\n",
      "  0.03557022 -0.09846417 -0.05577702  0.10882102 -0.01697011 -0.05473228\n",
      "  0.05849976  0.00668939 -0.0233738  -0.00994812  0.13825682  0.06140467\n",
      "  0.13699605  0.04800136 -0.11246715 -0.09195538  0.10368555 -0.02563272\n",
      " -0.15652139 -0.00697749 -0.0573875   0.03297834  0.07363617 -0.036808\n",
      " -0.01195372  0.07615831  0.05826311 -0.05425169  0.09715109 -0.10294102\n",
      " -0.0373436   0.06010899  0.00256366  0.09229365  0.00873205 -0.15475459\n",
      "  0.04454244  0.04248656 -0.017005    0.04898053 -0.07608194  0.08098866\n",
      " -0.0445774  -0.1420382  -0.04909434 -0.04198209  0.0206383   0.00901206\n",
      "  0.05583682  0.02709157  0.0069837  -0.02551496 -0.0697751   0.01004619\n",
      "  0.09747685  0.05175278 -0.00159748 -0.07268689 -0.12389037 -0.07475024\n",
      " -0.00084262  0.00533324  0.0362311   0.10937117]\n"
     ]
    }
   ],
   "source": [
    "# Print some example tweets and their vector reps for both views\n",
    "index = 70\n",
    "print alldocs['view1'][index]\n",
    "tag = alldocs['view1'][index].tags[0]\n",
    "print '\\n', simple_models['view1'][0].docvecs[tag]\n",
    "\n",
    "print '\\n', alldocs['view2'][index]\n",
    "print '\\n', simple_models['view2'][0].docvecs[tag]\n",
    "#print '\\n\\n', doc_list['view2'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center(data):\n",
    "    return data - np.mean(data, axis=0)\n",
    "\n",
    "def PLS(X, Y):\n",
    "    cross_cov = np.dot(center(X), center(Y).T)\n",
    "    eigval,eigvec=np.linalg.eig(cross_cov.dot(cross_cov.T))\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def PLS_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    accuracies = np.zeros((len(num_neighb), len(dims)))\n",
    "    # (score, dim, k, PLS_subspace, classifier_object)\n",
    "    best = (0, 0, 0, None, None) \n",
    "    #run pls\n",
    "    eigval, U = PLS(acoustic_train, artic_train)\n",
    "    for j, k in enumerate(num_neighb):\n",
    "        for i, d in enumerate(dims):\n",
    "            U_d = get_top_eigvec(eigval, U, d)\n",
    "            #get projection to pls space\n",
    "            train_proj = np.dot(U_d.T, acoustic_train_cen)\n",
    "            dev_proj = np.dot(U_d.T, acoustic_dev_cen)\n",
    "            # stack with mfcc39\n",
    "            stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "            stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "            \n",
    "            #classify\n",
    "            clf = neighbors.KNeighborsClassifier(k)\n",
    "            clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "            #predictions\n",
    "            score = clf.score(stacked_dev.T, phones_dev)\n",
    "            if score > best[0]:\n",
    "                best = (score, d, k, U_d, clf)\n",
    "            accuracies[j,i] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def CCA(X, Y, regX = 0, regY = 0):\n",
    "    cenX = center(X)\n",
    "    cenY = center(Y)\n",
    "    cross_cov = cenX.dot(cenY.T)\n",
    "    covX = cenX.dot(cenX.T)\n",
    "    covY = cenY.dot(cenY.T)\n",
    "    r_Ix = regX * np.eye(covX.shape[0])\n",
    "    r_Iy = regY * np.eye(covY.shape[0])\n",
    "    A = reduce(np.dot, [ np.linalg.inv(covX + r_Ix), cross_cov, np.linalg.inv(covY + r_Iy), cross_cov.T ])\n",
    "    eigval,eigvec=np.linalg.eig(A)\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def get_top_eigvec(eigval, eigvec, k):\n",
    "    idx=np.argsort(eigval)[-k:][::-1]\n",
    "    #eigval=eigval[idx]\n",
    "    return eigvec[:,idx]\n",
    "\n",
    "def CCA_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    reg = [1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1e1]\n",
    "    accuracies = np.zeros((len(reg), len(reg), len(dims), len(num_neighb)))\n",
    "    # (score, dim, regX, regY, k, CCA_subspace, classifier_object)\n",
    "    best = (0, 0, 0, 0, 0, None, None)\n",
    "    for rx, regX in enumerate(reg):\n",
    "        for ry, regY in enumerate(reg):              \n",
    "            #run cca\n",
    "            eigval, U = CCA(acoustic_train, artic_train, regX, regY)\n",
    "            for i, d in enumerate(dims):\n",
    "                U_d = get_top_eigvec(eigval, U, d)\n",
    "                #get projection to cca space\n",
    "                train_proj = U_d.T.dot(acoustic_train_cen)\n",
    "                dev_proj = U_d.T.dot(acoustic_dev_cen)\n",
    "                # stack with mfcc39\n",
    "                stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "                stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "                #classify\n",
    "                for j, k in enumerate(num_neighb):\n",
    "                    clf = neighbors.KNeighborsClassifier(k)\n",
    "                    clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "                    #predictions\n",
    "                    score = clf.score(stacked_dev.T, phones_dev)\n",
    "                    if score > best[0]:\n",
    "                        best = (score, d, regX, regY, k, U_d, clf)\n",
    "                    accuracies[rx, ry, i, j] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def plot_pc2(data, eigvec, phones_data):\n",
    "    #project to top 2 princ. comp.\n",
    "    data_proj = np.dot(np.transpose(eigvec), data)\n",
    "    data_proj_labels=[data_proj[:,np.where(phones_data==lbl)] for lbl in labels_dict.values()]\n",
    "    #Plot\n",
    "    cmap = plt.get_cmap('jet_r')\n",
    "    N=len(labels)\n",
    "    colors = [cmap(float(i)/N) for i in np.linspace(5.0, 0, N)]\n",
    "    plt.figure(figsize=(7,7))\n",
    "    #plt.subplot(2,1,1)\n",
    "    for i in range(N):\n",
    "        plt.scatter(data_proj_labels[i][0,:], data_proj_labels[i][1,:] ,c=colors[i], marker='+', label=labels[i]);\n",
    "    #plt.legend(plots,labels)\n",
    "    plt.legend(loc=3)\n",
    "    #plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select the best performing word2vec model\n",
    "best_model_name = 'dbow+dmc'\n",
    "best_alpha = 0.0022000\n",
    "best_model = { v1 : models_by_name[v1][best_model_name],\n",
    "              v2 : models_by_name[v2][best_model_name] }\n",
    "# Train best model\n",
    "shuffle(doc_list[view])\n",
    "for view in [v1, v2]:\n",
    "    best_model[v1].alpha, best_model[v1].min_alpha = best_alpha, best_alpha\n",
    "    best_model[v1].train(doc_list[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # DO CCA on the training docvecs\n",
    "# X = view 1, Y = view 2 : [num_samples x word_vec_size]\n",
    "target_sentiments, X, Y = zip(*[(doc.sentiment, best_model[v1].docvecs[doc.tags[0]], \\\n",
    "                             best_model[v2].docvecs[doc.tage[0]]) for doc in train_set])\n",
    "(cca_eigval, cca_eigvec) = CCA(X.T, Y.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
