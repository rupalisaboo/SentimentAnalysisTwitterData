{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/view1_clean not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Split the all_views, get one file(sentiment, tweet) for each view\n",
    "#Clean each view\n",
    "#Run this for each view.\n",
    "norm() {\n",
    "    fn=$1\n",
    "    if [ ! -f \"$fn\" ]\n",
    "    then\n",
    "        echo \"File: $fn not found\"\n",
    "        exit\n",
    "    fi\n",
    "    #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
    "    function normalize_text {\n",
    "        awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
    "        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
    "        -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
    "    }\n",
    "    export LC_ALL=C\n",
    "    normalize_text \"$fn\"\n",
    "    wc -l $fn\n",
    "    mv \"$fn\" \"$fn-norm\"\n",
    "}\n",
    "norm \"data/view1_clean\" #file name is\n",
    "norm \"data/view2_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "tw_view_1 = 'data/view1_clean-norm'\n",
    "tw_view_2 = 'data/view2_clean-norm'\n",
    "assert os.path.isfile(tw_view_1), tw_view_1 + \" unavailable\"\n",
    "assert os.path.isfile(tw_view_2), tw_view_2 + \" unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3805 docs: 2664 train-sentiment, 1141 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple, defaultdict as dd\n",
    "\n",
    "#sentiment = {'positive':1, 'negative':-1} #, 'neutral':2}\n",
    "sentiment_dict = {'4':1, '0':-1} #- new data 0,4\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = dd(list)  # will hold all docs in original order - dictionary, keys = [v1, v2]\n",
    "v1 = 'view1'\n",
    "v2 = 'view2'\n",
    "#tw_sentiment_dict = {}\n",
    "#print total_num, train_test_shuffle\n",
    "all_v2_words = []\n",
    "with open(tw_view_2) as allview2:\n",
    "        all_v2_words = allview2.readlines()\n",
    "total_num = len(all_v2_words)\n",
    "#split train/test\n",
    "train_num = total_num *  7 / 10 # 70% train/test 1 - 10\n",
    "train_test_shuffle = np.arange(total_num)\n",
    "np.random.shuffle(train_test_shuffle)\n",
    "with open(tw_view_1) as allview1:\n",
    "    #for line_no, (v1, v2) in enumerate(zip(allview1, allview2)):\n",
    "    for line_no, line in enumerate(allview1):\n",
    "        tokens = gensim.utils.to_unicode(line).split('\\t')\n",
    "        if len(tokens) != 2:\n",
    "            print line\n",
    "            raise Exception()\n",
    "        sentiment = sentiment_dict[tokens[0]]\n",
    "        #if tw_id not in tw_sentiment_dict.keys():\n",
    "        #    continue\n",
    "        words = tokens[1]\n",
    "        split = 'train' if train_test_shuffle[line_no] <= train_num else 'test'\n",
    "        #sentiment = tw_sentiment_dict[tw_id]\n",
    "        v2_words = gensim.utils.to_unicode(all_v2_words[line_no]).split('\\t')[1]\n",
    "        \n",
    "        alldocs[v1].append(SentimentDocument(words, [line_no], split, sentiment))\n",
    "        alldocs[v2].append(SentimentDocument(v2_words, [line_no], split, sentiment))\n",
    "train_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'train'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'train']\n",
    "}\n",
    "test_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'test'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'test']\n",
    "}\n",
    "doc_list = { v1: alldocs[v1][:], v2: alldocs[v2][:] }  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list[v1]), len(train_docs[v1]), len(test_docs[v1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view1 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view1 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view1 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view2 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "model_size = 200\n",
    "simple_models , models_by_name = {}, {} \n",
    "for view in [v1, v2]:\n",
    "    simple_models[view] = [\n",
    "        # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "        Doc2Vec(dm=1, dm_concat=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "        # PV-DBOW \n",
    "        Doc2Vec(dm=0, size=model_size, negative=5, hs=0, min_count=5, workers=cores),\n",
    "        # PV-DM w/average\n",
    "        Doc2Vec(dm=1, dm_mean=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    ]\n",
    "\n",
    "    # speed setup by sharing results of 1st model's vocabulary scan\n",
    "    simple_models[view][0].build_vocab(alldocs[view])  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "    print view, simple_models[view][0]\n",
    "    for model in simple_models[view][1:]:\n",
    "        model.reset_from(simple_models[view][0])\n",
    "        print view, model\n",
    "\n",
    "    models_by_name[view] = OrderedDict((str(model), model) for model in simple_models[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "for view in [v1, v2]:\n",
    "    models_by_name[view]['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][2]])\n",
    "    models_by_name[view]['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][0]])\n",
    "#print models_by_name['dbow+dmm'], models_by_name['dbow+dmc'] \n",
    "#del models_by_name['dbow+dmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn import svm, metrics, neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "import ipdb\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor(train_targets, train_regressors):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_regressors, train_targets)\n",
    "    return lr\n",
    "\n",
    "def svm_predictor(train_targets, train_regressors):\n",
    "    svc = svm.SVC(kernel='rbf', degree=5, gamma=1e-1)\n",
    "    svc.fit(train_regressors, train_targets)\n",
    "    return svc\n",
    "\n",
    "    \"\"\"expected = svm_y_test\n",
    "    predicted = svc.predict(svm_x_test)\n",
    "\n",
    "    #print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "    #      % (svc, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    \"\"\"\n",
    "def rf_predictor(train_targets, train_regressors):\n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc.fit(train_regressors, train_targets)\n",
    "    return rfc\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    #train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor(train_targets, train_regressors)\n",
    "    #predictor = svm_predictor(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_data]\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_data]\n",
    "    \"\"\"if not infer:\n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "              % (predictor, metrics.classification_report(expected, predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    corrects = sum(expected == predicted)\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "%matplotlib inline\n",
    "def center(data):\n",
    "    return data - np.mean(data, axis=0)\n",
    "\n",
    "def PLS(X, Y):\n",
    "    cross_cov = np.dot(center(X), center(Y).T)\n",
    "    eigval,eigvec=np.linalg.eig(cross_cov.dot(cross_cov.T))\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def PLS_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    accuracies = np.zeros((len(num_neighb), len(dims)))\n",
    "    # (score, dim, k, PLS_subspace, classifier_object)\n",
    "    best = (0, 0, 0, None, None)\n",
    "    #run pls\n",
    "    eigval, U = PLS(acoustic_train, artic_train)\n",
    "    for j, k in enumerate(num_neighb):\n",
    "        for i, d in enumerate(dims):\n",
    "            U_d = get_top_eigvec(eigval, U, d)\n",
    "            #get projection to pls space\n",
    "            train_proj = np.dot(U_d.T, acoustic_train_cen)\n",
    "            dev_proj = np.dot(U_d.T, acoustic_dev_cen)\n",
    "            # stack with mfcc39\n",
    "            stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "            stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "\n",
    "            #classify\n",
    "            clf = neighbors.KNeighborsClassifier(k)\n",
    "            clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "            #predictions\n",
    "            score = clf.score(stacked_dev.T, phones_dev)\n",
    "            if score > best[0]:\n",
    "                best = (score, d, k, U_d, clf)\n",
    "            accuracies[j,i] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def kcca(X, Y, regX=0.1, regY=0.1, numCC=10, kernelcca=True, ktype=\"gaussian\"):\n",
    "    '''Set up and solve the eigenproblem for the data in kernel and specified reg\n",
    "    '''\n",
    "    kernel1 = np.array(_make_kernel(X.T, ktype=ktype))\n",
    "    kernel_x = center_kernel(kernel1)\n",
    "    \n",
    "    print np.shape(kernel_x)\n",
    "    #kernel_x = (kernel1 + kernel1.T)/2\n",
    "    kernel2 = np.array(_make_kernel(Y.T, ktype=ktype))\n",
    "    kernel_y = center_kernel(kernel2)\n",
    "    print np.shape(kernel_y)\n",
    "    #kernel_y = (kernel2 + kernel2.T)/2\n",
    "    r_Ix = regX * np.eye(kernel_x.shape[0])\n",
    "    r_Iy = regY * np.eye(kernel_y.shape[0])\n",
    "    A = reduce(np.dot, [ np.linalg.inv(kernel_x - r_Ix), kernel_y, np.linalg.inv(kernel_y - r_Iy), kernel_x])\n",
    "    eigval,eigvec=np.linalg.eig(A)\n",
    "    return (eigval, eigvec, kernel_x, kernel_y)\n",
    "\n",
    "\n",
    "def center_kernel(K):\n",
    "    one_n = np.ones(np.shape(K)[0])\n",
    "    center_k = K - np.dot(one_n, K) - np.dot(K, one_n) + reduce(np.dot, [one_n, K, one_n])\n",
    "    return center_k\n",
    "\n",
    "def _make_kernel(d, normalize=True, ktype=\"gaussian\", sigma=1.0):\n",
    "    '''Makes a kernel for data d\n",
    "      If ktype is \"linear\", the kernel is a linear inner product\n",
    "      If ktype is \"gaussian\", the kernel is a Gaussian kernel with sigma = sigma\n",
    "    '''\n",
    "    if ktype == \"linear\":\n",
    "        d = np.nan_to_num(d)\n",
    "        cd = _demean(d)\n",
    "        kernel = np.dot(cd, cd.T)\n",
    "    elif ktype == \"gaussian\":\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        # this is an NxD matrix, where N is number of items and D its dimensionalites\n",
    "        pairwise_dists = squareform(pdist(d, 'euclidean'))\n",
    "        kernel = np.exp(-pairwise_dists ** 2 / sigma ** 2)\n",
    "    kernel = (kernel + kernel.T) / 2.\n",
    "    kernel = kernel / np.linalg.eigvalsh(kernel).max()\n",
    "    return kernel\n",
    "\n",
    "def _make_cross_kernel(d1, d2, normalize=True, ktype=\"gaussian\", sigma=1.0):\n",
    "    '''Makes a kernel for data d\n",
    "      If ktype is \"linear\", the kernel is a linear inner product\n",
    "      If ktype is \"gaussian\", the kernel is a Gaussian kernel with sigma = sigma\n",
    "    '''\n",
    "    if ktype == \"linear\":\n",
    "        d = np.nan_to_num(d)\n",
    "        cd = _demean(d)\n",
    "        kernel = np.dot(cd, cd.T)\n",
    "    elif ktype == \"gaussian\":\n",
    "        from scipy.spatial.distance import cdist, squareform\n",
    "        # this is an NxD matrix, where N is number of items and D its dimensionalites\n",
    "        pairwise_dists = cdist(d1, d2, 'euclidean')\n",
    "        kernel = np.exp(-pairwise_dists ** 2 / sigma ** 2)\n",
    "    #kernel = (kernel + kernel.T) / 2.\n",
    "    #ernel = kernel / np.linalg.eigvalsh(kernel).max()\n",
    "    return kernel\n",
    "\n",
    "def CCA(X, Y, regX = 0, regY = 0):\n",
    "    cenX = center(X)\n",
    "    cenY = center(Y)\n",
    "    cross_cov = cenX.dot(cenY.T)\n",
    "    covX = cenX.dot(cenX.T)\n",
    "    covY = cenY.dot(cenY.T)\n",
    "    r_Ix = regX * np.eye(covX.shape[0])\n",
    "    r_Iy = regY * np.eye(covY.shape[0])\n",
    "    A = reduce(np.dot, [ np.linalg.inv(covX + r_Ix), cross_cov, np.linalg.inv(covY + r_Iy), cross_cov.T ])\n",
    "    eigval,eigvec=np.linalg.eig(A)\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def get_top_eigvec(eigval, eigvec, k):\n",
    "    idx=np.argsort(eigval)[-k:][::-1]\n",
    "    #eigval=eigval[idx]\n",
    "    return eigvec[:,idx]\n",
    "\n",
    "def CCA_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    reg = [1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1e1]\n",
    "    accuracies = np.zeros((len(reg), len(reg), len(dims), len(num_neighb)))\n",
    "    # (score, dim, regX, regY, k, CCA_subspace, classifier_object)\n",
    "    best = (0, 0, 0, 0, 0, None, None)\n",
    "    for rx, regX in enumerate(reg):\n",
    "        for ry, regY in enumerate(reg):\n",
    "            #run cca\n",
    "            eigval, U = CCA(acoustic_train, artic_train, regX, regY)\n",
    "            for i, d in enumerate(dims):\n",
    "                U_d = get_top_eigvec(eigval, U, d)\n",
    "                #get projection to cca space\n",
    "                train_proj = U_d.T.dot(acoustic_train_cen)\n",
    "                dev_proj = U_d.T.dot(acoustic_dev_cen)\n",
    "                # stack with mfcc39\n",
    "                stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "                stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "                #classify\n",
    "                for j, k in enumerate(num_neighb):\n",
    "                    clf = neighbors.KNeighborsClassifier(k)\n",
    "                    clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "                    #predictions\n",
    "                    score = clf.score(stacked_dev.T, phones_dev)\n",
    "                    if score > best[0]:\n",
    "                        best = (score, d, regX, regY, k, U_d, clf)\n",
    "                    accuracies[rx, ry, i, j] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def plot_pc2(data, eigvec, phones_data):\n",
    "    #project to top 2 princ. comp.\n",
    "    data_proj = np.dot(np.transpose(eigvec), data)\n",
    "    data_proj_labels=[data_proj[:,np.where(phones_data==lbl)] for lbl in labels_dict.values()]\n",
    "    #Plot\n",
    "    cmap = plt.get_cmap('jet_r')\n",
    "    N=len(labels)\n",
    "    colors = [cmap(float(i)/N) for i in np.linspace(5.0, 0, N)]\n",
    "    plt.figure(figsize=(7,7))\n",
    "    #plt.subplot(2,1,1)\n",
    "    for i in range(N):\n",
    "        plt.scatter(data_proj_labels[i][0,:], data_proj_labels[i][1,:] ,c=colors[i], marker='+', label=labels[i]);\n",
    "    #plt.legend(plots,labels)\n",
    "    plt.legend(loc=3)\n",
    "    #plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "best_error = dd(lambda: dd(lambda :(1.0, 0.0))) # { view: { model_name : (error_rate, alpha) } } ,to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started.\n",
      "======== view1 =========\n",
      "START 2015-12-14 22:29:11.331988\n",
      "END 2015-12-14 22:29:46.118204\n",
      "======== view2 =========\n",
      "START 2015-12-14 22:29:46.118806\n",
      "END 2015-12-14 22:30:20.278070\n"
     ]
    }
   ],
   "source": [
    "predictor_alg = logistic_predictor\n",
    "#predictor_alg = svm_predictor\n",
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "print 'Started.'\n",
    "for view in [v1, v2]:\n",
    "    alpha, min_alpha, passes = (0.025, 0.001, 10)\n",
    "    alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "    print \"======== %s =========\" %view\n",
    "    print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "    for epoch in range(passes):\n",
    "        shuffle(doc_list[view])  # shuffling gets best results\n",
    "\n",
    "        for name, train_model in models_by_name[view].items():\n",
    "            #print name\n",
    "            # train\n",
    "            duration = 'na'\n",
    "            train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "            with elapsed_timer() as elapsed:\n",
    "                train_model.train(doc_list[view])\n",
    "                duration = '%.1f' % elapsed() \n",
    "\n",
    "            # evaluate\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view])\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if err < best_error[view][name][0]:\n",
    "                best_error[view][name] = (err, alpha)\n",
    "                best_indicator = '*' \n",
    "            #print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, err, epoch + 1, view, name, duration, eval_duration))\n",
    "\n",
    "            \"\"\"if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "                eval_duration = ''\n",
    "                with elapsed_timer() as eval_elapsed:\n",
    "                    infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view], infer=True)\n",
    "                eval_duration = '%.1f' % eval_elapsed()\n",
    "                best_indicator = ' '\n",
    "                if infer_err < best_error[view][name + '_inferred'][0]:\n",
    "                    best_error[view][name + '_inferred'] = (infer_err, alpha)\n",
    "                    best_indicator = '*'\n",
    "                print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, infer_err, epoch + 1, view, name + '_inferred', duration, eval_duration))\n",
    "\"\"\"\n",
    "        #print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "        alpha -= alpha_delta\n",
    "\n",
    "    print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= view1 ========\n",
      "0.370727 dbow+dmm 0.010600\n",
      "0.373357 dbow+dmc 0.008200\n",
      "0.379492 Doc2Vec(dbow,d200,n5,mc5,t4) 0.005800\n",
      "0.409290 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.015400\n",
      "0.416301 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.005800\n",
      "========= view2 ========\n",
      "0.432954 Doc2Vec(dbow,d200,n5,mc5,t4) 0.005800\n",
      "0.439089 dbow+dmc 0.013000\n",
      "0.447853 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.025000\n",
      "0.453988 dbow+dmm 0.015400\n",
      "0.462752 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.015400\n"
     ]
    }
   ],
   "source": [
    "for view in [v1, v2]:\n",
    "    print '========= %s ========' %view\n",
    "    for rate, alpha, name in sorted((rate, alpha, name) for name, (rate, alpha) in best_error[view].items()):\n",
    "        print(\"%f %s %f\" % (rate, name, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 2904...\n",
      "SentimentDocument(words=u'I wish I could be back in DC!!\\n', tags=[2904], split='train', sentiment=-1)\n",
      "\n",
      "SentimentDocument(words=u\"DON'T MESS WITH THE TITANS!!! COLLEGE WORLD SERIES BOUND BABY!!! P.S. My body is sore all over from today! Barry is amazing!!! ||Positive||\\n\", tags=[2904], split='train', sentiment=-1)\n",
      "Doc2Vec(dm/c,d200,n5,w3,mc2,t4):\n",
      " [(3390, 0.861107349395752), (3723, 0.776567280292511), (2904, 0.743794322013855)]\n",
      "Doc2Vec(dbow,d200,n5,mc5,t4):\n",
      " [(2904, 0.9087924957275391), (61, 0.8457372188568115), (1506, 0.7416622042655945)]\n",
      "Doc2Vec(dm/m,d200,n5,w3,mc2,t4):\n",
      " [(2904, 0.8184219598770142), (2195, 0.7294286489486694), (3488, 0.6891400218009949)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[v1][0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "# Print example tweet and vector reps for both views\n",
    "print alldocs['view1'][doc_id]\n",
    "#tag = alldocs['view1'][doc_id].tags[0]\n",
    "#print '\\n', simple_models['view1'][0].docvecs[tag]\n",
    "\n",
    "print '\\n', alldocs['view2'][doc_id]\n",
    "#print '\\n', simple_models['view2'][0].docvecs[tag]\n",
    "#print '\\n\\n', doc_list['view2'][:10]\n",
    "for model in simple_models[v1]:\n",
    "    inferred_docvec = model.infer_vector(alldocs[v1][doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 1084...\n",
      "Doc2Vec(dm/c,d200,n5,w3,mc2,t4):\n",
      " [(444, 0.8435599207878113), (1084, 0.8130500912666321), (347, 0.7728061079978943)]\n",
      "Doc2Vec(dbow,d200,n5,mc5,t4):\n",
      " [(1084, 0.919434130191803), (3459, 0.8288213014602661), (3498, 0.7632253766059875)]\n",
      "Doc2Vec(dm/m,d200,n5,w3,mc2,t4):\n",
      " [(1084, 0.836891770362854), (1073, 0.7253444194793701), (419, 0.7068144083023071)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[v1][0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models[v1]:\n",
    "    inferred_docvec = model.infer_vector(alldocs[v1][doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbow+dmm\n",
      "0.0106\n"
     ]
    }
   ],
   "source": [
    "#Select the best performing word2vec model\n",
    "_, best_alpha, best_model_name = min(((rate, alpha, name) \\\n",
    "                                           for name, (rate, alpha) in best_error[v1].items()), key=lambda b: b[0])\n",
    "print best_model_name \n",
    "print best_alpha\n",
    "best_model = { v1 : models_by_name[v1][best_model_name],\n",
    "              v2 : models_by_name[v2][best_model_name] }\n",
    "# Train best model\n",
    "shuffle(doc_list[view])\n",
    "for view in [v1, v2]:\n",
    "    best_model[view].alpha, best_model[view].min_alpha = best_alpha, best_alpha\n",
    "    best_model[view].train(doc_list[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO CCA on the training docvecs\n",
    "# X = view 1, Y = view 2 : [word_vec_size x num_samples]\n",
    "target_sentiments, X, Y = zip(*[(doc.sentiment, best_model[v1].docvecs[doc.tags[0]], \\\n",
    "                             best_model[v2].docvecs[doc.tags[0]]) for doc in train_docs[v1]])\n",
    "X = np.asarray(X).T\n",
    "Y = np.asarray(Y).T\n",
    "#test docs\n",
    "test = [best_model[v1].docvecs[doc.tags[0]] for doc in test_docs[v1]]\n",
    "test = np.asarray(test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2664, 2664)\n",
      "(2664, 2664)\n",
      "(400, 2664) (400, 2664)\n",
      "(2664, 2664)\n"
     ]
    }
   ],
   "source": [
    "(cca_eigval, cca_eigvec, kernel_x, kernel_y) = kcca(X, Y, regX=0.1, regY=0.1, numCC=10, kernelcca=True, ktype=\"gaussian\")\n",
    "print np.shape(X), np.shape(Y)\n",
    "print np.shape(cca_eigvec)\n",
    "#print np.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[270 241]\n",
      " [187 443]]\n",
      "0.375109553024\n"
     ]
    }
   ],
   "source": [
    "predictor = logistic_predictor(target_sentiments, X.T)\n",
    "\n",
    "# predict & evaluate\n",
    "test_predictions = predictor.predict(test.T)\n",
    "predicted = np.rint(test_predictions)\n",
    "expected = [doc.sentiment for doc in test_docs[v1]]\n",
    "#print(\"Classification report for %s:\\n%s\\n\" % (predictor, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "#ipdb.set_trace()\n",
    "errors = len(test_predictions) - sum(expected == predicted)\n",
    "err_orig = float(errors) / len(expected)\n",
    "print err_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get top k eigvec, project training data and stack with original word vectors\n",
    "err_cca = []\n",
    "step = 5\n",
    "num_dir_ranges = range(step, step+200, step)\n",
    "for num_dir in num_dir_ranges:\n",
    "    top_k_eigv = get_top_eigvec(cca_eigval, cca_eigvec, num_dir)\n",
    "    #print np.shape(top_k_eigv)\n",
    "    X_proj = top_k_eigv.T.dot(kernel_x)\n",
    "    #print np.shape(X_proj)\n",
    "    stacked_vec = np.append(X, X_proj, axis=0)\n",
    "    #print np.shape(stacked_vec)\n",
    "\n",
    "    # project test data to cca directions and stack\n",
    "    #print np.shape(test)\n",
    "    #print np.shape(test)\n",
    "    test_kernel = _make_cross_kernel(X.T, test.T, normalize=True, ktype=\"gaussian\", sigma=1.0)\n",
    "    test_proj = top_k_eigv.T.dot(test_kernel)\n",
    "    stacked_test = np.append(test, test_proj, axis=0)\n",
    "    #print np.shape(stacked_test)\n",
    "    #print np.shape(target_sentiments)\n",
    "\n",
    "    predictor = logistic_predictor(target_sentiments, stacked_vec.T)\n",
    "\n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(stacked_test.T)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_docs[v1]]\n",
    "    #print(\"Classification report for %s:\\n%s\\n\" % (predictor, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    #ipdb.set_trace()\n",
    "    errors = len(test_predictions) - sum(expected == predicted)\n",
    "    err_cca.append(float(errors) / len(expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEPCAYAAABV6CMBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTELT0sNtiqAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95264cc190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_dir_ranges, [err_orig]*len(num_dir_ranges), 'r--', num_dir_ranges, err_cca, 'b*')\n",
    "plt.ylabel('Mis-classification rate')\n",
    "plt.xlabel('Number of kCCA directions stacked with original sentence vectors')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
