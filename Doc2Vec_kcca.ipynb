{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/view1_clean not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Split the all_views, get one file(sentiment, tweet) for each view\n",
    "#Clean each view\n",
    "#Run this for each view.\n",
    "norm() {\n",
    "    fn=$1\n",
    "    if [ ! -f \"$fn\" ]\n",
    "    then\n",
    "        echo \"File: $fn not found\"\n",
    "        exit\n",
    "    fi\n",
    "    #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
    "    function normalize_text {\n",
    "        awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
    "        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
    "        -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
    "    }\n",
    "    export LC_ALL=C\n",
    "    normalize_text \"$fn\"\n",
    "    wc -l $fn\n",
    "    mv \"$fn\" \"$fn-norm\"\n",
    "}\n",
    "norm \"data/view1_clean\" #file name is\n",
    "norm \"data/view2_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "tw_view_1 = 'data/view1_clean-norm'\n",
    "tw_view_2 = 'data/view2_clean-norm'\n",
    "assert os.path.isfile(tw_view_1), tw_view_1 + \" unavailable\"\n",
    "assert os.path.isfile(tw_view_2), tw_view_2 + \" unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3805 docs: 2664 train-sentiment, 1141 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple, defaultdict as dd\n",
    "\n",
    "#sentiment = {'positive':1, 'negative':-1} #, 'neutral':2}\n",
    "sentiment_dict = {'4':1, '0':-1} #- new data 0,4\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = dd(list)  # will hold all docs in original order - dictionary, keys = [v1, v2]\n",
    "v1 = 'view1'\n",
    "v2 = 'view2'\n",
    "#tw_sentiment_dict = {}\n",
    "#print total_num, train_test_shuffle\n",
    "all_v2_words = []\n",
    "with open(tw_view_2) as allview2:\n",
    "        all_v2_words = allview2.readlines()\n",
    "total_num = len(all_v2_words)\n",
    "#split train/test\n",
    "train_num = total_num *  7 / 10 # 70% train/test 1 - 10\n",
    "train_test_shuffle = np.arange(total_num)\n",
    "np.random.shuffle(train_test_shuffle)\n",
    "with open(tw_view_1) as allview1:\n",
    "    #for line_no, (v1, v2) in enumerate(zip(allview1, allview2)):\n",
    "    for line_no, line in enumerate(allview1):\n",
    "        tokens = gensim.utils.to_unicode(line).split('\\t')\n",
    "        if len(tokens) != 2:\n",
    "            print line\n",
    "            raise Exception()\n",
    "        sentiment = sentiment_dict[tokens[0]]\n",
    "        #if tw_id not in tw_sentiment_dict.keys():\n",
    "        #    continue\n",
    "        words = tokens[1]\n",
    "        split = 'train' if train_test_shuffle[line_no] <= train_num else 'test'\n",
    "        #sentiment = tw_sentiment_dict[tw_id]\n",
    "        v2_words = gensim.utils.to_unicode(all_v2_words[line_no]).split('\\t')[1]\n",
    "        \n",
    "        alldocs[v1].append(SentimentDocument(words, [line_no], split, sentiment))\n",
    "        alldocs[v2].append(SentimentDocument(v2_words, [line_no], split, sentiment))\n",
    "train_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'train'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'train']\n",
    "}\n",
    "test_docs = {\n",
    "    v1 : [doc for doc in alldocs[v1] if doc.split == 'test'],\n",
    "    v2 : [doc for doc in alldocs[v2] if doc.split == 'test']\n",
    "}\n",
    "doc_list = { v1: alldocs[v1][:], v2: alldocs[v2][:] }  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list[v1]), len(train_docs[v1]), len(test_docs[v1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view1 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view1 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view1 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dm/c,d200,n5,w3,mc2,t4)\n",
      "view2 Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "view2 Doc2Vec(dm/m,d200,n5,w3,mc2,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "model_size = 200\n",
    "simple_models , models_by_name = {}, {} \n",
    "for view in [v1, v2]:\n",
    "    simple_models[view] = [\n",
    "        # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "        Doc2Vec(dm=1, dm_concat=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "        # PV-DBOW \n",
    "        Doc2Vec(dm=0, size=model_size, negative=5, hs=0, min_count=5, workers=cores),\n",
    "        # PV-DM w/average\n",
    "        Doc2Vec(dm=1, dm_mean=1, size=model_size, window=3, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    ]\n",
    "\n",
    "    # speed setup by sharing results of 1st model's vocabulary scan\n",
    "    simple_models[view][0].build_vocab(alldocs[view])  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "    print view, simple_models[view][0]\n",
    "    for model in simple_models[view][1:]:\n",
    "        model.reset_from(simple_models[view][0])\n",
    "        print view, model\n",
    "\n",
    "    models_by_name[view] = OrderedDict((str(model), model) for model in simple_models[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "for view in [v1, v2]:\n",
    "    models_by_name[view]['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][2]])\n",
    "    models_by_name[view]['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[view][1], simple_models[view][0]])\n",
    "#print models_by_name['dbow+dmm'], models_by_name['dbow+dmc'] \n",
    "#del models_by_name['dbow+dmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn import svm, metrics, neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "import ipdb\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor(train_targets, train_regressors):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_regressors, train_targets)\n",
    "    return lr\n",
    "\n",
    "def svm_predictor(train_targets, train_regressors):\n",
    "    svc = svm.SVC(kernel='rbf', degree=5, gamma=1e-1)\n",
    "    svc.fit(train_regressors, train_targets)\n",
    "    return svc\n",
    "\n",
    "    \"\"\"expected = svm_y_test\n",
    "    predicted = svc.predict(svm_x_test)\n",
    "\n",
    "    #print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "    #      % (svc, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    \"\"\"\n",
    "def rf_predictor(train_targets, train_regressors):\n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc.fit(train_regressors, train_targets)\n",
    "    return rfc\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    #train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor(train_targets, train_regressors)\n",
    "    #predictor = svm_predictor(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_data]\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_data]\n",
    "    \"\"\"if not infer:\n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "              % (predictor, metrics.classification_report(expected, predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    corrects = sum(expected == predicted)\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "%matplotlib inline\n",
    "def center(data):\n",
    "    return data - np.mean(data, axis=0)\n",
    "\n",
    "def PLS(X, Y):\n",
    "    cross_cov = np.dot(center(X), center(Y).T)\n",
    "    eigval,eigvec=np.linalg.eig(cross_cov.dot(cross_cov.T))\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def PLS_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    accuracies = np.zeros((len(num_neighb), len(dims)))\n",
    "    # (score, dim, k, PLS_subspace, classifier_object)\n",
    "    best = (0, 0, 0, None, None)\n",
    "    #run pls\n",
    "    eigval, U = PLS(acoustic_train, artic_train)\n",
    "    for j, k in enumerate(num_neighb):\n",
    "        for i, d in enumerate(dims):\n",
    "            U_d = get_top_eigvec(eigval, U, d)\n",
    "            #get projection to pls space\n",
    "            train_proj = np.dot(U_d.T, acoustic_train_cen)\n",
    "            dev_proj = np.dot(U_d.T, acoustic_dev_cen)\n",
    "            # stack with mfcc39\n",
    "            stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "            stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "\n",
    "            #classify\n",
    "            clf = neighbors.KNeighborsClassifier(k)\n",
    "            clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "            #predictions\n",
    "            score = clf.score(stacked_dev.T, phones_dev)\n",
    "            if score > best[0]:\n",
    "                best = (score, d, k, U_d, clf)\n",
    "            accuracies[j,i] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def kcca(X, Y, regX=0.1, regY=0.1, numCC=10, kernelcca=True, ktype=\"gaussian\"):\n",
    "    '''Set up and solve the eigenproblem for the data in kernel and specified reg\n",
    "    '''\n",
    "    cenX = center(X)\n",
    "    cenY = center(Y)\n",
    "    kernel1 = np.array([_make_kernel(X.T, ktype=ktype)])\n",
    "    kernel_x = (kernel1 + kernel1.T)/2\n",
    "    kernel2 = np.array([_make_kernel(Y.T, ktype=ktype)])\n",
    "    kernel_y = (kernel2 + kernel2.T)/2\n",
    "    r_Ix = regX * np.eye(kernel_x.shape[0])\n",
    "    r_Iy = regY * np.eye(kernel_y.shape[0])\n",
    "    A = reduce(np.dot, [ np.linalg.inv(kernel_x - r_Ix), kernel_y, np.linalg.inv(kernel_y - r_Iy), kernel_x])\n",
    "    eigval,eigvec=np.linalg.eig(A)\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "\n",
    "\n",
    "def _make_kernel(d, normalize=True, ktype=\"linear\", sigma=1.0):\n",
    "    '''Makes a kernel for data d\n",
    "      If ktype is \"linear\", the kernel is a linear inner product\n",
    "      If ktype is \"gaussian\", the kernel is a Gaussian kernel with sigma = sigma\n",
    "    '''\n",
    "    if ktype == \"linear\":\n",
    "        d = np.nan_to_num(d)\n",
    "        cd = _demean(d)\n",
    "        kernel = np.dot(cd, cd.T)\n",
    "    elif ktype == \"gaussian\":\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        # this is an NxD matrix, where N is number of items and D its dimensionalites\n",
    "        pairwise_dists = squareform(pdist(d, 'euclidean'))\n",
    "        kernel = np.exp(-pairwise_dists ** 2 / sigma ** 2)\n",
    "    kernel = (kernel + kernel.T) / 2.\n",
    "    kernel = kernel / np.linalg.eigvalsh(kernel).max()\n",
    "    return kernel\n",
    "\n",
    "def CCA(X, Y, regX = 0, regY = 0):\n",
    "    cenX = center(X)\n",
    "    cenY = center(Y)\n",
    "    cross_cov = cenX.dot(cenY.T)\n",
    "    covX = cenX.dot(cenX.T)\n",
    "    covY = cenY.dot(cenY.T)\n",
    "    r_Ix = regX * np.eye(covX.shape[0])\n",
    "    r_Iy = regY * np.eye(covY.shape[0])\n",
    "    A = reduce(np.dot, [ np.linalg.inv(covX + r_Ix), cross_cov, np.linalg.inv(covY + r_Iy), cross_cov.T ])\n",
    "    eigval,eigvec=np.linalg.eig(A)\n",
    "    return (eigval, eigvec)\n",
    "\n",
    "def get_top_eigvec(eigval, eigvec, k):\n",
    "    idx=np.argsort(eigval)[-k:][::-1]\n",
    "    #eigval=eigval[idx]\n",
    "    return eigvec[:,idx]\n",
    "\n",
    "def CCA_MFCC():\n",
    "    dims = [10, 30, 50, 70, 90, 110]\n",
    "    reg = [1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1e1]\n",
    "    accuracies = np.zeros((len(reg), len(reg), len(dims), len(num_neighb)))\n",
    "    # (score, dim, regX, regY, k, CCA_subspace, classifier_object)\n",
    "    best = (0, 0, 0, 0, 0, None, None)\n",
    "    for rx, regX in enumerate(reg):\n",
    "        for ry, regY in enumerate(reg):\n",
    "            #run cca\n",
    "            eigval, U = CCA(acoustic_train, artic_train, regX, regY)\n",
    "            for i, d in enumerate(dims):\n",
    "                U_d = get_top_eigvec(eigval, U, d)\n",
    "                #get projection to cca space\n",
    "                train_proj = U_d.T.dot(acoustic_train_cen)\n",
    "                dev_proj = U_d.T.dot(acoustic_dev_cen)\n",
    "                # stack with mfcc39\n",
    "                stacked_train = np.append(train_proj, mfcc39_train, axis=0)\n",
    "                stacked_dev = np.append(dev_proj, mfcc39_dev, axis=0)\n",
    "                #classify\n",
    "                for j, k in enumerate(num_neighb):\n",
    "                    clf = neighbors.KNeighborsClassifier(k)\n",
    "                    clf.fit(stacked_train.T, phones_train)\n",
    "\n",
    "                    #predictions\n",
    "                    score = clf.score(stacked_dev.T, phones_dev)\n",
    "                    if score > best[0]:\n",
    "                        best = (score, d, regX, regY, k, U_d, clf)\n",
    "                    accuracies[rx, ry, i, j] = score\n",
    "    return (best, accuracies)\n",
    "\n",
    "def plot_pc2(data, eigvec, phones_data):\n",
    "    #project to top 2 princ. comp.\n",
    "    data_proj = np.dot(np.transpose(eigvec), data)\n",
    "    data_proj_labels=[data_proj[:,np.where(phones_data==lbl)] for lbl in labels_dict.values()]\n",
    "    #Plot\n",
    "    cmap = plt.get_cmap('jet_r')\n",
    "    N=len(labels)\n",
    "    colors = [cmap(float(i)/N) for i in np.linspace(5.0, 0, N)]\n",
    "    plt.figure(figsize=(7,7))\n",
    "    #plt.subplot(2,1,1)\n",
    "    for i in range(N):\n",
    "        plt.scatter(data_proj_labels[i][0,:], data_proj_labels[i][1,:] ,c=colors[i], marker='+', label=labels[i]);\n",
    "    #plt.legend(plots,labels)\n",
    "    plt.legend(loc=3)\n",
    "    #plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-84a222dda7b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from collections import defaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# { view: { model_name : (error_rate, alpha) } } ,to selectively-print only best errors achieved\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dd' is not defined"
     ]
    }
   ],
   "source": [
    "#from collections import defaultdict\n",
    "best_error = dd(lambda: dd(lambda :(1.0, 0.0))) # { view: { model_name : (error_rate, alpha) } } ,to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logistic_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ce81c2e1af8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictor_alg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_predictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#predictor_alg = svm_predictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logistic_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "predictor_alg = logistic_predictor\n",
    "#predictor_alg = svm_predictor\n",
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "print 'Started.'\n",
    "for view in [v1, v2]:\n",
    "    alpha, min_alpha, passes = (0.025, 0.001, 10)\n",
    "    alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "    print \"======== %s =========\" %view\n",
    "    print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "    for epoch in range(passes):\n",
    "        shuffle(doc_list[view])  # shuffling gets best results\n",
    "\n",
    "        for name, train_model in models_by_name[view].items():\n",
    "            #print name\n",
    "            # train\n",
    "            duration = 'na'\n",
    "            train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "            with elapsed_timer() as elapsed:\n",
    "                train_model.train(doc_list[view])\n",
    "                duration = '%.1f' % elapsed()\n",
    "\n",
    "            # evaluate\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view])\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if err < best_error[view][name][0]:\n",
    "                best_error[view][name] = (err, alpha)\n",
    "                best_indicator = '*' \n",
    "            #print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, err, epoch + 1, view, name, duration, eval_duration))\n",
    "\n",
    "            \"\"\"if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "                eval_duration = ''\n",
    "                with elapsed_timer() as eval_elapsed:\n",
    "                    infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs[view], test_docs[view], infer=True)\n",
    "                eval_duration = '%.1f' % eval_elapsed()\n",
    "                best_indicator = ' '\n",
    "                if infer_err < best_error[view][name + '_inferred'][0]:\n",
    "                    best_error[view][name + '_inferred'] = (infer_err, alpha)\n",
    "                    best_indicator = '*'\n",
    "                print(\"%s%f : %i passes : %s-%s %ss %ss\" % (best_indicator, infer_err, epoch + 1, view, name + '_inferred', duration, eval_duration))\n",
    "\"\"\"\n",
    "        #print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "        alpha -= alpha_delta\n",
    "\n",
    "    print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= view1 ========\n",
      "0.379492 Doc2Vec(dbow,d200,n5,mc5,t4) 0.005800\n",
      "0.382997 dbow+dmm 0.025000\n",
      "0.384750 dbow+dmc 0.015400\n",
      "0.402279 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.003400\n",
      "0.424189 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.013000\n",
      "========= view2 ========\n",
      "0.428571 Doc2Vec(dbow,d200,n5,mc5,t4) 0.022600\n",
      "0.447853 Doc2Vec(dm/m,d200,n5,w3,mc2,t4) 0.003400\n",
      "0.448729 dbow+dmm 0.017800\n",
      "0.451358 dbow+dmc 0.022600\n",
      "0.480280 Doc2Vec(dm/c,d200,n5,w3,mc2,t4) 0.025000\n"
     ]
    }
   ],
   "source": [
    "for view in [v1, v2]:\n",
    "    print '========= %s ========' %view\n",
    "    for rate, alpha, name in sorted((rate, alpha, name) for name, (rate, alpha) in best_error[view].items()):\n",
    "        print(\"%f %s %f\" % (rate, name, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 1251...\n",
      "SentimentDocument(words=u'Stuck in traffic.\\n', tags=[1251], split='test', sentiment=-1)\n",
      "\n",
      "SentimentDocument(words=u'Need to work off those jack in the box tacos !\\n', tags=[1251], split='test', sentiment=-1)\n",
      "Doc2Vec(dm/c,d200,n5,w3,mc2,t4):\n",
      " [(1251, 0.8739808797836304), (165, 0.6244778037071228), (574, 0.5938631892204285)]\n",
      "Doc2Vec(dbow,d200,n5,mc5,t4):\n",
      " [(1251, 0.8624033331871033), (3067, 0.6627092957496643), (476, 0.6534361243247986)]\n",
      "Doc2Vec(dm/m,d200,n5,w3,mc2,t4):\n",
      " [(1251, 0.7922778129577637), (3067, 0.7556555867195129), (81, 0.7104895114898682)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[v1][0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "# Print example tweet and vector reps for both views\n",
    "print alldocs['view1'][doc_id]\n",
    "#tag = alldocs['view1'][doc_id].tags[0]\n",
    "#print '\\n', simple_models['view1'][0].docvecs[tag]\n",
    "\n",
    "print '\\n', alldocs['view2'][doc_id]\n",
    "#print '\\n', simple_models['view2'][0].docvecs[tag]\n",
    "#print '\\n\\n', doc_list['view2'][:10]\n",
    "for model in simple_models[v1]:\n",
    "    inferred_docvec = model.infer_vector(alldocs[v1][doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 288...\n",
      "Doc2Vec(dm/c,d200,n5,w3,mc2,t4):\n",
      " [(288, 0.8648703098297119), (3577, 0.637226402759552), (3424, 0.5923616886138916)]\n",
      "Doc2Vec(dbow,d200,n5,mc5,t4):\n",
      " [(288, 0.9358938932418823), (3004, 0.5766725540161133), (1923, 0.528923511505127)]\n",
      "Doc2Vec(dm/m,d200,n5,w3,mc2,t4):\n",
      " [(288, 0.8433932662010193), (3425, 0.6562464237213135), (3357, 0.6520817875862122)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[v1][0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models[v1]:\n",
    "    inferred_docvec = model.infer_vector(alldocs[v1][doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d200,n5,mc5,t4)\n",
      "0.0058\n"
     ]
    }
   ],
   "source": [
    "#Select the best performing word2vec model\n",
    "_, best_alpha, best_model_name = min(((rate, alpha, name) \\\n",
    "                                           for name, (rate, alpha) in best_error[v1].items()), key=lambda b: b[0])\n",
    "print best_model_name \n",
    "print best_alpha\n",
    "best_model = { v1 : models_by_name[v1][best_model_name],\n",
    "              v2 : models_by_name[v2][best_model_name] }\n",
    "# Train best model\n",
    "shuffle(doc_list[view])\n",
    "for view in [v1, v2]:\n",
    "    best_model[view].alpha, best_model[view].min_alpha = best_alpha, best_alpha\n",
    "    best_model[view].train(doc_list[view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO CCA on the training docvecs\n",
    "# X = view 1, Y = view 2 : [word_vec_size x num_samples]\n",
    "target_sentiments, X, Y = zip(*[(doc.sentiment, best_model[v1].docvecs[doc.tags[0]], \\\n",
    "                             best_model[v2].docvecs[doc.tags[0]]) for doc in train_docs[v1]])\n",
    "X = np.asarray(X).T\n",
    "Y = np.asarray(Y).T\n",
    "#test docs\n",
    "test = [best_model[v1].docvecs[doc.tags[0]] for doc in test_docs[v1]]\n",
    "test = np.asarray(test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(cca_eigval, cca_eigvec) = kcca(X, Y, regX=0.1, regY=0.1, numCC=10, kernelcca=True, ktype=\"gaussian\")\n",
    "print np.shape(X), np.shape(Y)\n",
    "print np.shape(cca_eigvec)\n",
    "#print np.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[282 261]\n",
      " [173 425]]\n",
      "0.38036809816\n"
     ]
    }
   ],
   "source": [
    "predictor = logistic_predictor(target_sentiments, X.T)\n",
    "\n",
    "# predict & evaluate\n",
    "test_predictions = predictor.predict(test.T)\n",
    "predicted = np.rint(test_predictions)\n",
    "expected = [doc.sentiment for doc in test_docs[v1]]\n",
    "#print(\"Classification report for %s:\\n%s\\n\" % (predictor, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "#ipdb.set_trace()\n",
    "errors = len(test_predictions) - sum(expected == predicted)\n",
    "err_orig = float(errors) / len(expected)\n",
    "print err_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,400) and (200,2664) not aligned: 400 (dim 1) != 200 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3777255d41b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtop_k_eigv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_top_eigvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcca_eigval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcca_eigvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print np.shape(top_k_eigv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mX_proj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_k_eigv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m#print np.shape(X_proj)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstacked_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_proj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,400) and (200,2664) not aligned: 400 (dim 1) != 200 (dim 0)"
     ]
    }
   ],
   "source": [
    "#get top k eigvec, project training data and stack with original word vectors\n",
    "err_cca = []\n",
    "step = 5\n",
    "num_dir_ranges = range(step, step+200, step)\n",
    "for num_dir in num_dir_ranges:\n",
    "    top_k_eigv = get_top_eigvec(cca_eigval, cca_eigvec, num_dir)\n",
    "    #print np.shape(top_k_eigv)\n",
    "    X_proj = top_k_eigv.T.dot(X)\n",
    "    #print np.shape(X_proj)\n",
    "    stacked_vec = np.append(X, X_proj, axis=0)\n",
    "    #print np.shape(stacked_vec)\n",
    "\n",
    "    # project test data to cca directions and stack\n",
    "    #print np.shape(test)\n",
    "    test_proj = top_k_eigv.T.dot(test)\n",
    "    stacked_test = np.append(test, test_proj, axis=0)\n",
    "    #print np.shape(stacked_test)\n",
    "    #print np.shape(target_sentiments)\n",
    "\n",
    "    predictor = logistic_predictor(target_sentiments, stacked_vec.T)\n",
    "\n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(stacked_test.T)\n",
    "    predicted = np.rint(test_predictions)\n",
    "    expected = [doc.sentiment for doc in test_docs[v1]]\n",
    "    #print(\"Classification report for %s:\\n%s\\n\" % (predictor, metrics.classification_report(expected, predicted)))\n",
    "    #print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    #ipdb.set_trace()\n",
    "    errors = len(test_predictions) - sum(expected == predicted)\n",
    "    err_cca.append(float(errors) / len(expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEKCAYAAAArYJMgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f44ccb7f410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_dir_ranges, [err_orig]*len(num_dir_ranges), 'r--', num_dir_ranges, err_cca, 'b*')\n",
    "plt.ylabel('Mis-classification rate')\n",
    "plt.xlabel('Number of CCA directions stacked with original sentence vectors')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
